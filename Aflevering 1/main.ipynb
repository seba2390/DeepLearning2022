{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from myDataSet import *\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "class DataPrep:\n",
    "    def __init__(self, dataset: MyCustomImageDataset,\n",
    "                    batch_size: int = 20,\n",
    "                 data_fraction: float = 0.1,\n",
    "                 test_fraction: float = 0.2) -> None:\n",
    "        self.X, self.Y = dataset.X.to(torch.float32), list(dataset.Y)\n",
    "        self.batch_size = batch_size\n",
    "        self.test_fraction = test_fraction\n",
    "        self.data_fraction = data_fraction\n",
    "\n",
    "        self.label_dict = None\n",
    "        self.train_X, self.train_Y = None, None\n",
    "        self.test_X, self.test_Y   = None, None\n",
    "\n",
    "\n",
    "    def prepare(self):\n",
    "        # One-hot encoding labels\n",
    "        self.Y = self.one_hot(self.Y)\n",
    "\n",
    "        # Normalizing pixels values\n",
    "        self.X = self.normalize_pixels(self.X)\n",
    "\n",
    "        # Batching data in batches\n",
    "        self.X, self.Y = self.batch_data(data=self.X,\n",
    "                                       labels=self.Y,\n",
    "                                   batch_size=self.batch_size,\n",
    "                                    randomize=True)\n",
    "\n",
    "        # Picking out portion size according to 'data_fraction'\n",
    "        print(\"Initial nr. of batches: \", len(self.X))\n",
    "        self.X, self.Y = self.X[:int(self.data_fraction*len(self.X))], self.Y[:int(self.data_fraction*len(self.Y))]\n",
    "        print(\"Final nr. of batches: \", len(self.X))\n",
    "\n",
    "        # Splitting into test and training set according to 'test_fraction'\n",
    "        self.test_X, self.test_Y = self.X[:int(self.test_fraction*len(self.X))], self.Y[:int(self.test_fraction*len(self.Y))]\n",
    "        self.train_X, self.train_Y = self.X[len(self.test_X):], self.Y[len(self.test_Y):]\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_data(data: torch.Tensor,\n",
    "                   labels: torch.Tensor,\n",
    "                   batch_size: int,\n",
    "                   randomize: bool = False) -> tuple[list[torch.Tensor],list[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Takes the input data, shuffles it randomly and repacks it in batches of\n",
    "        'batch_size'. If len(labels) % batch_size != 0, the final number of\n",
    "        batches is rounded down.\n",
    "\n",
    "         Parameters:\n",
    "        - data: A torch Tensor of shape (nr. data points, nr. channels, height, width).\n",
    "        - labels: Either tuple, list or 1d numpy array containing the labels for the data.\n",
    "        - batch_size: The number of data points contained within each batch.\n",
    "        - randomize: A boolean determining whether the data is randomly shuffled.\n",
    "\n",
    "         Returns:\n",
    "        - batches: A tuple of (X_batches, Y_batches)\n",
    "        \"\"\"\n",
    "        assert type(data) is torch.Tensor and len(data.shape) == 4\n",
    "        assert type(labels) is torch.Tensor and len(labels.shape) == 2\n",
    "\n",
    "        _indices = [i for i in range(len(labels))]\n",
    "        if randomize:\n",
    "            _indices = torch.randperm(len(_indices))\n",
    "            _indices = _indices.tolist()\n",
    "\n",
    "        _X_batches, _Y_batches = [], []\n",
    "        for _batch in range(len(labels) // batch_size):\n",
    "            _i= _indices[batch_size*_batch : batch_size*(_batch+1)]\n",
    "            _X_batches.append(data[_i,::])\n",
    "            _Y_batches.append(labels[_i,::])\n",
    "\n",
    "        batches = _X_batches, _Y_batches\n",
    "        return batches\n",
    "\n",
    "    def one_hot(self, labels: tuple | list) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Creates a one-hot encoding of given labels.\n",
    "\n",
    "         Parameters:\n",
    "        - labels: tuple or list containing labels.\n",
    "\n",
    "         Returns:\n",
    "        - one_hot_Y: 2D torch Tensor of shape (nr. data points, nr. classes).\n",
    "        \"\"\"\n",
    "        assert type(labels) is tuple or type(labels) is list, f'Unrecognized labels type: should be tuple or list.'\n",
    "        # Creating map from label to integer\n",
    "        self.label_dict = {}\n",
    "        for class_idx, label in enumerate(set(labels)):\n",
    "            self.label_dict[label] = class_idx\n",
    "        # Calculating Number of data points x Nr of classes one-hot encoding of Y\n",
    "        nr_labels = len(list(labels))\n",
    "        nr_unique_labels = len(list(self.label_dict.keys()))\n",
    "        one_hot_Y = torch.zeros(size=(nr_labels, nr_unique_labels))\n",
    "        for i, label in enumerate(labels):\n",
    "            one_hot_Y[i][self.label_dict[label]] = 1\n",
    "        return one_hot_Y\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_pixels(data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Void type function normalizing every value: (0,255) -> (0,1), in input tensor.\n",
    "\n",
    "         Parameters:\n",
    "        - data: 4D torch tensor of data of shape (nr. datapoints, nr. channels, height, width)\n",
    "\n",
    "        \"\"\"\n",
    "        _PIXEL_MAX = 255\n",
    "        for data_point in range(data.shape[0]):\n",
    "            data[data_point] *= 1.0/_PIXEL_MAX\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    \"\"\" Inspired by: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\"\"\"\n",
    "    def __init__(self, channels_in: int,\n",
    "                        nr_classes: int,\n",
    "                 input_dimensions: tuple[int,int]) -> None:\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_dim = input_dimensions\n",
    "        self.nr_classes = nr_classes\n",
    "        self.device = None\n",
    "\n",
    "        # layer 1: convolutional layer\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=channels_in,\n",
    "                                    out_channels=6,\n",
    "                                     kernel_size=(5,5),\n",
    "                                          stride=(1,1),\n",
    "                                         padding=(0,0))\n",
    "        # activation function\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        object_dim1 = self.output_dim(self.input_dim,\n",
    "                                      self.conv1.kernel_size,\n",
    "                                      self.conv1.stride,\n",
    "                                      self.conv1.padding)\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 2: pooling layer\n",
    "        self.pool1 = torch.nn.AvgPool2d(kernel_size=(2,2),\n",
    "                                             stride=(1,1),\n",
    "                                             padding=(0,0))\n",
    "\n",
    "        object_dim2 = self.output_dim(object_dim1,\n",
    "                                      self.pool1.kernel_size,\n",
    "                                      self.pool1.stride,\n",
    "                                      self.pool1.padding)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 3: convolutional layer\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=6,\n",
    "                                    out_channels=16,\n",
    "                                     kernel_size=5,\n",
    "                                          stride=(1,1),\n",
    "                                         padding=(0,0))\n",
    "        # activation function\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        object_dim3 = self.output_dim(object_dim2,\n",
    "                                      self.conv2.kernel_size,\n",
    "                                      self.conv2.stride,\n",
    "                                      self.conv2.padding)\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 4: pooling layer\n",
    "        self.pool2 = torch.nn.AvgPool2d(kernel_size=(2,2),\n",
    "                                             stride=(1,1),\n",
    "                                             padding=(0,0))\n",
    "\n",
    "        object_dim4 = self.output_dim(object_dim3,\n",
    "                                      self.pool2.kernel_size,\n",
    "                                      self.pool2.stride,\n",
    "                                      self.pool2.padding)\n",
    "        # flattening\n",
    "        self.flatten = torch.nn.Flatten(start_dim=1,end_dim=-1)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 5: fully connected layer (lazy means input dim is automatically inferred)\n",
    "        in_vector_length = self.conv2.out_channels*object_dim4[0]*object_dim4[1]\n",
    "        self.lin1 = torch.nn.Linear(in_features=in_vector_length,\n",
    "                                    out_features=120)\n",
    "\n",
    "        # activation function\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 6: fully connected layer (lazy means input dim is automatically inferred)\n",
    "        self.lin2 = torch.nn.Linear(in_features=self.lin1.out_features,\n",
    "                                    out_features=self.nr_classes)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def output_dim(data_size: tuple[int, int],\n",
    "                   kernel_size: tuple[int, int],\n",
    "                   stride_size: tuple[int, int] = (1,1),\n",
    "                   padding_size: tuple[int, int] = (0,0)) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Calculates output shape of array after convolution w. specific\n",
    "        kernel, padding and stride.\n",
    "\n",
    "         Parameters:\n",
    "        - data_size: tuple containing dimension of 2D input array.\n",
    "        - kernel_size: tuple containing dimension of 2D kernel.\n",
    "        - stride_size: tuple containing dimension of 2D stride.\n",
    "        - padding_size: tuple containing dimension of 2D padding.\n",
    "\n",
    "         Returns:\n",
    "        - output_dimensions: tuple containing dimension of resulting 2D array.\n",
    "        \"\"\"\n",
    "\n",
    "        out_height = ((data_size[0] - kernel_size[0] + 2 * padding_size[0]) // stride_size[0]) + 1\n",
    "        out_width = ((data_size[1] - kernel_size[1] + 2 * padding_size[1]) // stride_size[1]) + 1\n",
    "\n",
    "        output_dimensions = (out_height, out_width)\n",
    "        return output_dimensions\n",
    "\n",
    "    def predict(self, forwarded_data:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Takes the raw estimate from a forward pass (nr. data points, nr. classes)\n",
    "        and sets the highest val in each row 1 and the rest zero.\n",
    "\n",
    "         Parameters:\n",
    "        - forwarded_data: torch.Tensor of shape (nr. data points, nr. classes)\n",
    "\n",
    "         Returns:\n",
    "        - _prediction: torc.Tensor of shape (nr. data points, nr. classes)\n",
    "\n",
    "        \"\"\"\n",
    "        assert forwarded_data.shape[1] == self.nr_classes, f'Forwarded data should be of shape (nr. data points, nr. classes).'\n",
    "        assert len(forwarded_data.shape) == 2, f' Forwarded data should be a 2D tensor.'\n",
    "        assert forwarded_data.device.type == self.device.type, f'forwarded_data is on device: {forwarded_data.device.type}, but should be on: {self.device.type}'\n",
    "        for _row in forwarded_data:\n",
    "            _row[_row < torch.max(_row)] = 0\n",
    "            _row[_row == torch.max(_row)] = 1\n",
    "            _row.to(self.device)\n",
    "        assert forwarded_data.device.type == self.device.type, f'_prediction is on device: {forwarded_data.device.type}, but should be on: {self.device.type}'\n",
    "        return forwarded_data\n",
    "\n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Function performing the forward pass on 'data', i.e. sequentially\n",
    "        applying each layer in the network.\n",
    "\n",
    "         Parameters:\n",
    "        - data: 4D torch tensor of shape (nr. data points, nr. channels, height, width).\n",
    "\n",
    "         Returns:\n",
    "        - data: 4D torch tensor of shape (nr. data points, nr. channels, height, width).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(data.shape) == 4, f'Shape of X should be (nr. data points, nr. channels, height, width)'\n",
    "        data = self.conv1(data)\n",
    "        data = self.activation1(data)\n",
    "        data = self.pool1(data)\n",
    "        data = self.conv2(data)\n",
    "        data = self.activation2(data)\n",
    "        data = self.pool2(data)\n",
    "        data = self.flatten(data)\n",
    "        data = self.lin1(data)\n",
    "        data = self.activation3(data)\n",
    "        data = self.lin2(data)\n",
    "        return data\n",
    "\n",
    "    def accuracy(self,y_hat: torch.Tensor,\n",
    "                   y_actual: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Function for calculating the accuracy of 'y_hat' against 'y_actual'\n",
    "        in terms of: nr. agreements / nr. comparisons.\n",
    "\n",
    "         Parameters:\n",
    "        - y_hat: 2D torch tensor of shape (nr. datapoints, nr. classes) only containing 1 and 0.\n",
    "        - y_actual: 2D torch tensor of shape (nr. datapoints, nr. classes) only containing 1 and 0.\n",
    "\n",
    "         Returns:\n",
    "        - _accuracy: float in range (0,1).\n",
    "        \"\"\"\n",
    "        assert y_hat.shape == y_actual.shape, f'Inputs are not of matching shapes.'\n",
    "        assert y_hat.device.type == self.device.type, f'y_hat is on device: {y_hat.device.type}, but should be on: {self.device.type}'\n",
    "        assert y_actual.device.type == self.device.type, f'y_actual is on device: {y_actual.device.type}, but should be on: {self.device.type}'\n",
    "\n",
    "\n",
    "        _nr_equals = torch.sum(y_hat*y_actual)\n",
    "        _accuracy = _nr_equals/y_hat.shape[0]\n",
    "        return _accuracy\n",
    "\n",
    "    def train_network(self,train_data_batches: list[torch.Tensor],\n",
    "                         train_labels_batches: list[torch.Tensor],\n",
    "                            test_data_batches: list[torch.Tensor],\n",
    "                          test_labels_batches: list[torch.Tensor],\n",
    "                                       epochs: int = 10,\n",
    "                                  device_name: str = \"cuda\") -> tuple[list[float],list[float],list[float],list[float]]:\n",
    "\n",
    "        \"\"\"\n",
    "        Function for training the network, e.i. learn the optimal weights for each\n",
    "        layer. Training is done using stochastic gradient descent, with Negative\n",
    "        Log-Likelihood (also known as Cross Entropy) as a loss function.\n",
    "\n",
    "         Parameters:\n",
    "        - train_data_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - train_labels_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - test_data_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - test_labels_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - epochs: nr. of epochs (iteration) used to train.\n",
    "\n",
    "         Returns:\n",
    "        - _train_accuracies: list of floats w. avg. accuracies of training data at given epoch.\n",
    "        - _train_losses: list of floats w. avg. loss of training data at given epoch.\n",
    "        - _test_accuracies: list of floats w. avg. accuracies of test data at given epoch.\n",
    "        - _test_losses: list of floats w. avg. loss of test data at given epoch.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        if device_name != 'cpu':\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(device_name)\n",
    "                # Allocating model weights on GPU\n",
    "                self.to(self.device)\n",
    "                self.train()\n",
    "                print(\"Current torch cuda version: \", torch.version.cuda)\n",
    "                print(\"Cuda devices found by torch: \", torch.cuda.device_count())\n",
    "                print(\"Current cuda device to be used by torch: \", torch.cuda.current_device())\n",
    "                print(\"Name of cuda device: \", torch.cuda.get_device_name(0))\n",
    "                t = torch.cuda.get_device_properties(0).total_memory\n",
    "                print(\"Total memory in cuda device: \", t/1e6, \"MB\")\n",
    "                r = torch.cuda.memory_reserved(0)\n",
    "                print(\"Total memory reserved in cuda device: \", r/1e6, \"MB\")\n",
    "                a = torch.cuda.memory_allocated(0)\n",
    "                print(\"Total memory allocated in cuda device: \", a/1e6, \"MB\")\n",
    "                print(\"Total remaining memory in cuda device:: \", (t-r)/1e6, \"MB\")\n",
    "                # Allocating input on GPU\n",
    "                for _batch in range(len(train_data_batches)):\n",
    "                    train_data_batches[_batch]   = train_data_batches[_batch].to(self.device)\n",
    "                    train_labels_batches[_batch] = train_labels_batches[_batch].to(self.device)\n",
    "                for _batch in range(len(test_data_batches)):\n",
    "                    test_data_batches[_batch]   = test_data_batches[_batch].to(self.device)\n",
    "                    test_labels_batches[_batch] = test_labels_batches[_batch].to(self.device)\n",
    "            else:\n",
    "                self.to(self.device)\n",
    "                self.train()\n",
    "                print(\"No cuda device available to torch - defaulting to cpu.\")\n",
    "\n",
    "        _optimizer = torch.optim.SGD(params=self.parameters(),\n",
    "                                     lr=0.001,\n",
    "                                     momentum=0.9)\n",
    "\n",
    "        _loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        _train_accuracies, _train_losses = [], []\n",
    "        _test_accuracies, _test_losses = [], []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            _counter = 0\n",
    "            _batch_train_acc, _batch_train_loss = 0.0, 0.0\n",
    "            _batch_test_acc,  _batch_test_loss = 0.0, 0.0\n",
    "            # Training against training data\n",
    "            for x_batch, y_batch in zip(train_data_batches,train_labels_batches):\n",
    "                assert x_batch.device.type == self.device.type, f'x_batch is on device: {x_batch.device.type}, but should be on: {self.device.type}'\n",
    "                assert y_batch.device.type == self.device.type, f'y_batch is on device: {y_batch.device.type}, but should be on: {self.device.type}'\n",
    "                # Ensure that parameter gradients are 0.\n",
    "                _optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                _y_hat = self.forward(x_batch)\n",
    "                assert _y_hat.shape[0] == train_labels_batches[0].shape[0]\n",
    "                assert _y_hat.shape[1] == self.nr_classes\n",
    "                # Loss calculation + backprop + optimization\n",
    "                _loss = _loss_function(_y_hat,y_batch)\n",
    "                _loss.backward()\n",
    "                _optimizer.step()\n",
    "                # Saving losses and accuracies\n",
    "                with torch.no_grad():\n",
    "                    if self.device.type != 'cpu':\n",
    "                        _batch_train_acc  += self.accuracy(self.predict(_y_hat),y_batch).item()\n",
    "                    else:\n",
    "                        _batch_train_acc  += self.accuracy(self.predict(_y_hat),y_batch)\n",
    "                    _batch_train_loss += _loss.item()\n",
    "                    _counter += 1\n",
    "            _train_accuracies.append(_batch_train_acc/_counter)\n",
    "            _train_losses.append(_batch_train_loss/_counter)\n",
    "            # Testing model on test data\n",
    "            for x_batch, y_batch in zip(test_data_batches,test_labels_batches):\n",
    "                assert x_batch.device.type == self.device.type, f'x_batch is on device: {x_batch.device.type}, but should be on: {self.device.type}'\n",
    "                assert y_batch.device.type == self.device.type, f'y_batch is on device: {y_batch.device.type}, but should be on: {self.device.type}'\n",
    "                # Forward pass\n",
    "                _y_hat = self.forward(x_batch)\n",
    "                assert _y_hat.shape[0] == train_labels_batches[0].shape[0]\n",
    "                assert _y_hat.shape[1] == self.nr_classes\n",
    "                # Loss calculation\n",
    "                _loss = _loss_function(_y_hat,y_batch)\n",
    "                # Saving losses and accuracies\n",
    "                with torch.no_grad():\n",
    "                    if self.device.type != 'cpu':\n",
    "                        _batch_test_acc  += self.accuracy(self.predict(_y_hat),y_batch).item()\n",
    "                    else:\n",
    "                        _batch_test_acc  += self.accuracy(self.predict(_y_hat),y_batch)\n",
    "                    _batch_test_loss += _loss.item()\n",
    "            _test_accuracies.append(_batch_test_acc/_counter)\n",
    "            _test_losses.append(_batch_test_loss/_counter)\n",
    "\n",
    "        # Removing from GPU and Allocating input on CPU\n",
    "        if self.device.type == \"cuda\":\n",
    "            self.device = torch.device('cpu')\n",
    "            self.to(self.device)\n",
    "            for _batch in range(len(train_data_batches)):\n",
    "                train_data_batches[_batch]   = train_data_batches[_batch].to(self.device)\n",
    "                train_labels_batches[_batch] = train_labels_batches[_batch].to(self.device)\n",
    "            for _batch in range(len(test_data_batches)):\n",
    "                test_data_batches[_batch]   = test_data_batches[_batch].to(self.device)\n",
    "                test_labels_batches[_batch] = test_labels_batches[_batch].to(self.device)\n",
    "\n",
    "        return _train_accuracies, _train_losses, _test_accuracies, _test_losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "class NeuralNet2(torch.nn.Module):\n",
    "    \"\"\" Inspired by https://learn.microsoft.com/en-us/windows/ai/windows-ml/tutorials/pytorch-train-model \"\"\"\n",
    "\n",
    "    def __init__(self, channels_in: int,\n",
    "                        nr_classes: int,\n",
    "                 input_dimensions: tuple[int,int]) -> None:\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.input_dim = input_dimensions\n",
    "        self.nr_classes = nr_classes\n",
    "        self.device = None\n",
    "\n",
    "        # layer 1: convolutional layer\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=channels_in,\n",
    "                                    out_channels=12,\n",
    "                                     kernel_size=(5,5),\n",
    "                                          stride=(1,1),\n",
    "                                         padding=(1,1))\n",
    "        # normalization function\n",
    "        self.norm1 = torch.nn.BatchNorm2d(self.conv1.out_channels)\n",
    "        # activation function\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        object_dim1 = self.output_dim(self.input_dim,\n",
    "                                      self.conv1.kernel_size,\n",
    "                                      self.conv1.stride,\n",
    "                                      self.conv1.padding)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 2: convolutional layer\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=self.conv1.out_channels,\n",
    "                                    out_channels=12,\n",
    "                                     kernel_size=(5,5),\n",
    "                                          stride=(1,1),\n",
    "                                         padding=(1,1))\n",
    "        # normalization function\n",
    "        self.norm2 = torch.nn.BatchNorm2d(self.conv2.out_channels)\n",
    "        # activation function\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        object_dim2 = self.output_dim(object_dim1,\n",
    "                                      self.conv2.kernel_size,\n",
    "                                      self.conv2.stride,\n",
    "                                      self.conv2.padding)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 3: pooling layer\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=(2,2),\n",
    "                                             stride=(2,2),\n",
    "                                             padding=(0,0))\n",
    "\n",
    "        object_dim3 = self.output_dim(object_dim2,\n",
    "                                      self.pool1.kernel_size,\n",
    "                                      self.pool1.stride,\n",
    "                                      self.pool1.padding)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 4: convolutional layer\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=self.conv2.out_channels,\n",
    "                                    out_channels=24,\n",
    "                                     kernel_size=(5,5),\n",
    "                                          stride=(1,1),\n",
    "                                         padding=(1,1))\n",
    "        # normalization function\n",
    "        self.norm3 = torch.nn.BatchNorm2d(self.conv3.out_channels)\n",
    "        # activation function\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        object_dim4 = self.output_dim(object_dim3,\n",
    "                                      self.conv3.kernel_size,\n",
    "                                      self.conv3.stride,\n",
    "                                      self.conv3.padding)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 5: convolutional layer\n",
    "        self.conv4 = torch.nn.Conv2d(in_channels=self.conv3.out_channels,\n",
    "                                    out_channels=24,\n",
    "                                     kernel_size=(5,5),\n",
    "                                          stride=(1,1),\n",
    "                                         padding=(1,1))\n",
    "        # normalization function\n",
    "        self.norm4= torch.nn.BatchNorm2d(self.conv4.out_channels)\n",
    "\n",
    "        # activation function\n",
    "        self.activation4 = torch.nn.ReLU()\n",
    "        object_dim5 = self.output_dim(object_dim4,\n",
    "                                      self.conv4.kernel_size,\n",
    "                                      self.conv4.stride,\n",
    "                                      self.conv4.padding)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # flattening\n",
    "        self.flatten = torch.nn.Flatten(start_dim=1,end_dim=-1)\n",
    "\n",
    "        #--------------------------------------#\n",
    "        # layer 6: fully connected layer (lazy means input dim is automatically inferred)\n",
    "        in_vector_length = self.conv4.out_channels*object_dim5[0]*object_dim5[1]\n",
    "        self.lin1 = torch.nn.Linear(in_features=in_vector_length,\n",
    "                                    out_features=self.nr_classes)\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def output_dim(data_size: tuple[int, int],\n",
    "                   kernel_size: tuple[int, int],\n",
    "                   stride_size: tuple[int, int] = (1,1),\n",
    "                   padding_size: tuple[int, int] = (0,0)) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Calculates output shape of array after convolution w. specific\n",
    "        kernel, padding and stride.\n",
    "\n",
    "         Parameters:\n",
    "        - data_size: tuple containing dimension of 2D input array.\n",
    "        - kernel_size: tuple containing dimension of 2D kernel.\n",
    "        - stride_size: tuple containing dimension of 2D stride.\n",
    "        - padding_size: tuple containing dimension of 2D padding.\n",
    "\n",
    "         Returns:\n",
    "        - output_dimensions: tuple containing dimension of resulting 2D array.\n",
    "        \"\"\"\n",
    "\n",
    "        out_height = ((data_size[0] - kernel_size[0] + 2 * padding_size[0]) // stride_size[0]) + 1\n",
    "        out_width = ((data_size[1] - kernel_size[1] + 2 * padding_size[1]) // stride_size[1]) + 1\n",
    "\n",
    "        output_dimensions = (out_height, out_width)\n",
    "        return output_dimensions\n",
    "\n",
    "    def predict(self, forwarded_data:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Takes the raw estimate from a forward pass (nr. data points, nr. classes)\n",
    "        and sets the highest val in each row 1 and the rest zero.\n",
    "\n",
    "         Parameters:\n",
    "        - forwarded_data: torch.Tensor of shape (nr. data points, nr. classes)\n",
    "\n",
    "         Returns:\n",
    "        - _prediction: torc.Tensor of shape (nr. data points, nr. classes)\n",
    "\n",
    "        \"\"\"\n",
    "        assert forwarded_data.shape[1] == self.nr_classes, f'Forwarded data should be of shape (nr. data points, nr. classes).'\n",
    "        assert len(forwarded_data.shape) == 2, f' Forwarded data should be a 2D tensor.'\n",
    "        assert forwarded_data.device.type == self.device.type, f'forwarded_data is on device: {forwarded_data.device.type}, but should be on: {self.device.type}'\n",
    "        for _row in forwarded_data:\n",
    "            _row[_row < torch.max(_row)] = 0\n",
    "            _row[_row == torch.max(_row)] = 1\n",
    "            _row.to(self.device)\n",
    "        assert forwarded_data.device.type == self.device.type, f'_prediction is on device: {forwarded_data.device.type}, but should be on: {self.device.type}'\n",
    "        return forwarded_data\n",
    "\n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Function performing the forward pass on 'data', i.e. sequentially\n",
    "        applying each layer in the network.\n",
    "\n",
    "         Parameters:\n",
    "        - data: 4D torch tensor of shape (nr. data points, nr. channels, height, width).\n",
    "\n",
    "         Returns:\n",
    "        - data: 4D torch tensor of shape (nr. data points, nr. channels, height, width).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(data.shape) == 4, f'Shape of X should be (nr. data points, nr. channels, height, width)'\n",
    "\n",
    "        # Layer 1\n",
    "        data = self.conv1(data)\n",
    "        data = self.norm1(data)\n",
    "        data = self.activation1(data)\n",
    "\n",
    "        # Layer 2\n",
    "        data = self.conv2(data)\n",
    "        data = self.norm2(data)\n",
    "        data = self.activation2(data)\n",
    "\n",
    "        # Pool layer\n",
    "        data = self.pool1(data)\n",
    "\n",
    "        # Layer 3\n",
    "        data = self.conv3(data)\n",
    "        data = self.norm3(data)\n",
    "        data = self.activation3(data)\n",
    "\n",
    "        # Layer 4\n",
    "        data = self.conv4(data)\n",
    "        data = self.norm4(data)\n",
    "        data = self.activation4(data)\n",
    "\n",
    "        # Layer 5\n",
    "        data = self.flatten(data)\n",
    "        data = self.lin1(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def accuracy(self,y_hat: torch.Tensor,\n",
    "                   y_actual: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Function for calculating the accuracy of 'y_hat' against 'y_actual'\n",
    "        in terms of: nr. agreements / nr. comparisons.\n",
    "\n",
    "         Parameters:\n",
    "        - y_hat: 2D torch tensor of shape (nr. datapoints, nr. classes) only containing 1 and 0.\n",
    "        - y_actual: 2D torch tensor of shape (nr. datapoints, nr. classes) only containing 1 and 0.\n",
    "\n",
    "         Returns:\n",
    "        - _accuracy: float in range (0,1).\n",
    "        \"\"\"\n",
    "        assert y_hat.shape == y_actual.shape, f'Inputs are not of matching shapes.'\n",
    "        assert y_hat.device.type == self.device.type, f'y_hat is on device: {y_hat.device.type}, but should be on: {self.device.type}'\n",
    "        assert y_actual.device.type == self.device.type, f'y_actual is on device: {y_actual.device.type}, but should be on: {self.device.type}'\n",
    "\n",
    "\n",
    "        _nr_equals = torch.sum(y_hat*y_actual)\n",
    "        _accuracy = _nr_equals/y_hat.shape[0]\n",
    "        return _accuracy\n",
    "\n",
    "    def train_network(self,train_data_batches: list[torch.Tensor],\n",
    "                         train_labels_batches: list[torch.Tensor],\n",
    "                            test_data_batches: list[torch.Tensor],\n",
    "                          test_labels_batches: list[torch.Tensor],\n",
    "                                       epochs: int = 10,\n",
    "                                  device_name: str = \"cuda\") -> tuple[list[float],list[float],list[float],list[float]]:\n",
    "\n",
    "        \"\"\"\n",
    "        Function for training the network, e.i. learn the optimal weights for each\n",
    "        layer. Training is done using stochastic gradient descent, with Negative\n",
    "        Log-Likelihood (also known as Cross Entropy) as a loss function.\n",
    "\n",
    "         Parameters:\n",
    "        - train_data_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - train_labels_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - test_data_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - test_labels_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - epochs: nr. of epochs (iteration) used to train.\n",
    "\n",
    "         Returns:\n",
    "        - _train_accuracies: list of floats w. avg. accuracies of training data at given epoch.\n",
    "        - _train_losses: list of floats w. avg. loss of training data at given epoch.\n",
    "        - _test_accuracies: list of floats w. avg. accuracies of test data at given epoch.\n",
    "        - _test_losses: list of floats w. avg. loss of test data at given epoch.\n",
    "\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cpu')\n",
    "        if device_name != 'cpu':\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(device_name)\n",
    "                # Allocating model weights on GPU\n",
    "                self.to(self.device)\n",
    "                self.train()\n",
    "                print(\"Current torch cuda version: \", torch.version.cuda)\n",
    "                print(\"Cuda devices found by torch: \", torch.cuda.device_count())\n",
    "                print(\"Current cuda device to be used by torch: \", torch.cuda.current_device())\n",
    "                print(\"Name of cuda device: \", torch.cuda.get_device_name(0))\n",
    "                t = torch.cuda.get_device_properties(0).total_memory\n",
    "                print(\"Total memory in cuda device: \", t/1e6, \"MB\")\n",
    "                r = torch.cuda.memory_reserved(0)\n",
    "                print(\"Total memory reserved in cuda device: \", r/1e6, \"MB\")\n",
    "                a = torch.cuda.memory_allocated(0)\n",
    "                print(\"Total memory allocated in cuda device: \", a/1e6, \"MB\")\n",
    "                print(\"Total remaining memory in cuda device:: \", (t-r)/1e6, \"MB\")\n",
    "                # Allocating input on GPU\n",
    "                for _batch in range(len(train_data_batches)):\n",
    "                    train_data_batches[_batch]   = train_data_batches[_batch].to(self.device)\n",
    "                    train_labels_batches[_batch] = train_labels_batches[_batch].to(self.device)\n",
    "                for _batch in range(len(test_data_batches)):\n",
    "                    test_data_batches[_batch]   = test_data_batches[_batch].to(self.device)\n",
    "                    test_labels_batches[_batch] = test_labels_batches[_batch].to(self.device)\n",
    "            else:\n",
    "                self.to(self.device)\n",
    "                self.train()\n",
    "                print(\"No cuda device available to torch - defaulting to cpu.\")\n",
    "\n",
    "\n",
    "        _optimizer = torch.optim.Adam(params=self.parameters(),\n",
    "                                     lr=0.001,\n",
    "                                     weight_decay=0.0001)\n",
    "        _loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        _train_accuracies, _train_losses = [], []\n",
    "        _test_accuracies, _test_losses = [], []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            _counter = 0\n",
    "            _batch_train_acc, _batch_train_loss = 0.0, 0.0\n",
    "            _batch_test_acc,  _batch_test_loss = 0.0, 0.0\n",
    "            # Training against training data\n",
    "            for x_batch, y_batch in zip(train_data_batches,train_labels_batches):\n",
    "                assert x_batch.device.type == self.device.type, f'x_batch is on device: {x_batch.device.type}, but should be on: {self.device.type}'\n",
    "                assert y_batch.device.type == self.device.type, f'y_batch is on device: {y_batch.device.type}, but should be on: {self.device.type}'\n",
    "                # Ensure that parameter gradients are 0.\n",
    "                _optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                _y_hat = self.forward(x_batch)\n",
    "                assert _y_hat.shape[0] == train_labels_batches[0].shape[0]\n",
    "                assert _y_hat.shape[1] == self.nr_classes\n",
    "                # Loss calculation + backprop + optimization\n",
    "                _loss = _loss_function(_y_hat,y_batch)\n",
    "                _loss.backward()\n",
    "                _optimizer.step()\n",
    "                # Saving losses and accuracies\n",
    "                with torch.no_grad():\n",
    "                    if self.device.type != 'cpu':\n",
    "                        _batch_train_acc  += self.accuracy(self.predict(_y_hat),y_batch).item()\n",
    "                    else:\n",
    "                        _batch_train_acc  += self.accuracy(self.predict(_y_hat),y_batch)\n",
    "                    _batch_train_loss += _loss.item()\n",
    "                    _counter += 1\n",
    "            _train_accuracies.append(_batch_train_acc/_counter)\n",
    "            _train_losses.append(_batch_train_loss/_counter)\n",
    "            # Testing model on test data\n",
    "            for x_batch, y_batch in zip(test_data_batches,test_labels_batches):\n",
    "                assert x_batch.device.type == self.device.type, f'x_batch is on device: {x_batch.device.type}, but should be on: {self.device.type}'\n",
    "                assert y_batch.device.type == self.device.type, f'y_batch is on device: {y_batch.device.type}, but should be on: {self.device.type}'\n",
    "                # Forward pass\n",
    "                _y_hat = self.forward(x_batch)\n",
    "                assert _y_hat.shape[0] == train_labels_batches[0].shape[0]\n",
    "                assert _y_hat.shape[1] == self.nr_classes\n",
    "                # Loss calculation\n",
    "                _loss = _loss_function(_y_hat,y_batch)\n",
    "                # Saving losses and accuracies\n",
    "                with torch.no_grad():\n",
    "                    if self.device.type != 'cpu':\n",
    "                        _batch_test_acc  += self.accuracy(self.predict(_y_hat),y_batch).item()\n",
    "                    else:\n",
    "                        _batch_test_acc  += self.accuracy(self.predict(_y_hat),y_batch)\n",
    "                    _batch_test_loss += _loss.item()\n",
    "            _test_accuracies.append(_batch_test_acc/_counter)\n",
    "            _test_losses.append(_batch_test_loss/_counter)\n",
    "\n",
    "        # Removing from GPU and Allocating input on CPU\n",
    "        if self.device.type == \"cuda\":\n",
    "            self.device = torch.device('cpu')\n",
    "            self.to(self.device)\n",
    "            for _batch in range(len(train_data_batches)):\n",
    "                train_data_batches[_batch]   = train_data_batches[_batch].to(self.device)\n",
    "                train_labels_batches[_batch] = train_labels_batches[_batch].to(self.device)\n",
    "            for _batch in range(len(test_data_batches)):\n",
    "                test_data_batches[_batch]   = test_data_batches[_batch].to(self.device)\n",
    "                test_labels_batches[_batch] = test_labels_batches[_batch].to(self.device)\n",
    "        return _train_accuracies, _train_losses, _test_accuracies, _test_losses\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial nr. of batches:  362\n",
      "Final nr. of batches:  25\n",
      "Shape of single batch:  torch.Size([2, 3, 520, 520])\n",
      "my_X_test_batches:  7\n"
     ]
    }
   ],
   "source": [
    "# Set up the dataset.\n",
    "image_directory = \"Insects\"\n",
    "annotations_file_directory = \"insects.csv\"\n",
    "dataset = MyCustomImageDataset(annotations_file_directory, image_directory)\n",
    "\n",
    "\n",
    "# Preparing data for learning (normalization, one-hot encoding and batching)\n",
    "my_batch_size, my_data_fraction, my_test_fraction = 2, 0.07, 0.3\n",
    "prepped_data = DataPrep(dataset=dataset,\n",
    "                        batch_size=my_batch_size,\n",
    "                        data_fraction=my_data_fraction,\n",
    "                        test_fraction=my_test_fraction)\n",
    "prepped_data.prepare()\n",
    "my_X_train_batches, my_Y_train_batches = prepped_data.train_X, prepped_data.train_Y\n",
    "my_X_test_batches, my_Y_test_batches = prepped_data.test_X, prepped_data.test_Y\n",
    "print(\"Shape of single batch: \",my_X_train_batches[0].shape)\n",
    "print(\"my_X_test_batches: \", len(my_X_test_batches))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "# Creating instance of neural network\n",
    "my_nr_channels = my_X_train_batches[0].shape[1]\n",
    "my_nr_classes = my_Y_train_batches[0].shape[1]\n",
    "my_input_dimensions = (my_X_train_batches[0].shape[2], my_X_train_batches[0].shape[3])\n",
    "\n",
    "my_net = NeuralNet2(channels_in=my_nr_channels,\n",
    "                   nr_classes=my_nr_classes,\n",
    "                   input_dimensions=my_input_dimensions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cuda device available to torch - defaulting to cpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [199], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_accuracies, train_losses, test_accuracies, test_losses \u001B[38;5;241m=\u001B[39m \u001B[43mmy_net\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_network\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                                                        \u001B[49m\u001B[43mtrain_data_batches\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmy_X_train_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                                                      \u001B[49m\u001B[43mtrain_labels_batches\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmy_Y_train_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                                                         \u001B[49m\u001B[43mtest_data_batches\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmy_X_test_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                                                       \u001B[49m\u001B[43mtest_labels_batches\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmy_Y_test_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                                                                    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [196], line 297\u001B[0m, in \u001B[0;36mNeuralNet2.train_network\u001B[0;34m(self, train_data_batches, train_labels_batches, test_data_batches, test_labels_batches, epochs, device_name)\u001B[0m\n\u001B[1;32m    295\u001B[0m \u001B[38;5;66;03m# Loss calculation + backprop + optimization\u001B[39;00m\n\u001B[1;32m    296\u001B[0m _loss \u001B[38;5;241m=\u001B[39m _loss_function(_y_hat,y_batch)\n\u001B[0;32m--> 297\u001B[0m \u001B[43m_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    298\u001B[0m _optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m    299\u001B[0m \u001B[38;5;66;03m# Saving losses and accuracies\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/torch/_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_accuracies, train_losses, test_accuracies, test_losses = my_net.train_network(\n",
    "                                                        train_data_batches=my_X_train_batches,\n",
    "                                                      train_labels_batches=my_Y_train_batches,\n",
    "                                                         test_data_batches=my_X_test_batches,\n",
    "                                                       test_labels_batches=my_Y_test_batches,\n",
    "                                                                    epochs=15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 600x400 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(6,4))\n",
    "epochs = [i+1 for i in range(len(train_accuracies))]\n",
    "ax.plot(epochs,train_accuracies,label=\"train acc\")\n",
    "#ax.plot(epochs,train_losses,label=\"train loss\")\n",
    "ax.plot(epochs,test_accuracies,label=\"test acc\")\n",
    "#ax.plot(epochs,test_losses,label=\"test loss\")\n",
    "ax.set_ylim(-0.1,1.2)\n",
    "ax.set_xlabel(\"Epoch nr.\")\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "def test_model(model: NeuralNet,\n",
    "               data: DataPrep,\n",
    "               nr_batches: int) -> None:\n",
    "    # Generating random indices\n",
    "    rand_indices = []\n",
    "    while len(rand_indices) < nr_batches:\n",
    "        _rand_int = torch.randint(low=0,high=len(data.Y),size=(1,)).item()\n",
    "        if _rand_int not in rand_indices:\n",
    "            rand_indices.append(_rand_int)\n",
    "    # Predicting w. model and checking against true labels\n",
    "    batch_counter = 0\n",
    "    correct_counter = 0\n",
    "    with torch.no_grad():\n",
    "        for _index in rand_indices:\n",
    "            mod = model.forward(data.X[_index])\n",
    "            y_hat  = model.predict(mod)\n",
    "            y_real = data.Y[_index]\n",
    "            print(\"\\n ---  Random Batch: \", batch_counter + 1, \" ---\")\n",
    "            for _data_point in range(y_hat.shape[0]):\n",
    "                pred = list(data.label_dict.keys())[torch.where(y_hat[_data_point]==1)[0]]\n",
    "                actual = list(data.label_dict.keys())[torch.where(y_real[_data_point]==1)[0]]\n",
    "                print(\"  ##| Prediction: \", pred,\" |--| Actual: \",actual,\" |##\")\n",
    "                if pred == actual:\n",
    "                    correct_counter += 1\n",
    "            batch_counter += 1\n",
    "    print(\"\\n #####| \", correct_counter, \"/\", batch_counter * data.batch_size, \" predicted correctly |#####\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---  Random Batch:  1  ---\n",
      "  ##| Prediction:  Lasioglossum punctatissimum  |--| Actual:  Lasioglossum punctatissimum  |##\n",
      "  ##| Prediction:  Lasioglossum punctatissimum  |--| Actual:  Andrena fulva  |##\n",
      "\n",
      " ---  Random Batch:  2  ---\n",
      "  ##| Prediction:  Lasioglossum punctatissimum  |--| Actual:  Lasioglossum punctatissimum  |##\n",
      "  ##| Prediction:  Panurgus banksianus  |--| Actual:  Panurgus banksianus  |##\n",
      "\n",
      " ---  Random Batch:  3  ---\n",
      "  ##| Prediction:  Lasioglossum punctatissimum  |--| Actual:  Lasioglossum punctatissimum  |##\n",
      "  ##| Prediction:  Panurgus banksianus  |--| Actual:  Panurgus banksianus  |##\n",
      "\n",
      " #####|  5 / 6  predicted correctly |#####\n"
     ]
    }
   ],
   "source": [
    "test_model(my_net,prepped_data,3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
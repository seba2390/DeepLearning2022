{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from myDataSet import *\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# TODO: Implement GPU memory alloc in case of CUDA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class DataPrep:\n",
    "    def __init__(self, dataset: MyCustomImageDataset,\n",
    "                    batch_size: int = 20,\n",
    "                 data_fraction: float = 0.1,\n",
    "                 test_fraction: float = 0.2) -> None:\n",
    "        self.X, self.Y = dataset.X.to(torch.float32), list(dataset.Y)\n",
    "        self.batch_size = batch_size\n",
    "        self.test_fraction = test_fraction\n",
    "        self.data_fraction = data_fraction\n",
    "\n",
    "        self.label_dict = None\n",
    "        self.train_X, self.train_Y = None, None\n",
    "        self.test_X, self.test_Y   = None, None\n",
    "\n",
    "\n",
    "    def prepare(self):\n",
    "        # One-hot encoding labels\n",
    "        self.Y = self.one_hot(self.Y)\n",
    "\n",
    "        # Normalizing pixels values\n",
    "        self.X = self.normalize_pixels(self.X)\n",
    "\n",
    "        # Batching data in batches\n",
    "        self.X, self.Y = self.batch_data(data=self.X,\n",
    "                                       labels=self.Y,\n",
    "                                   batch_size=self.batch_size,\n",
    "                                    randomize=True)\n",
    "\n",
    "        # Picking out portion size according to 'data_fraction'\n",
    "        print(\"Initial nr. of batches: \", len(self.X))\n",
    "        self.X, self.Y = self.X[:int(self.data_fraction*len(self.X))], self.Y[:int(self.data_fraction*len(self.Y))]\n",
    "        print(\"Final nr. of batches: \", len(self.X))\n",
    "\n",
    "        # Splitting into test and training set according to 'test_fraction'\n",
    "        self.test_X, self.test_Y = self.X[:int(self.test_fraction*len(self.X))], self.Y[:int(self.test_fraction*len(self.Y))]\n",
    "        self.train_X, self.train_Y = self.X[len(self.test_X):], self.Y[len(self.test_Y):]\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_data(data: torch.Tensor,\n",
    "                   labels: torch.Tensor,\n",
    "                   batch_size: int,\n",
    "                   randomize: bool = False) -> tuple[list[torch.Tensor],list[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Takes the input data, shuffles it randomly and repacks it in batches of\n",
    "        'batch_size'. If len(labels) % batch_size != 0, the final number of\n",
    "        batches is rounded down.\n",
    "\n",
    "         Parameters:\n",
    "        - data: A torch Tensor of shape (nr. data points, nr. channels, height, width).\n",
    "        - labels: Either tuple, list or 1d numpy array containing the labels for the data.\n",
    "        - batch_size: The number of data points contained within each batch.\n",
    "        - randomize: A boolean determining whether the data is randomly shuffled.\n",
    "\n",
    "         Returns:\n",
    "        - batches: A tuple of (X_batches, Y_batches)\n",
    "        \"\"\"\n",
    "        assert type(data) is torch.Tensor and len(data.shape) == 4\n",
    "        assert type(labels) is torch.Tensor and len(labels.shape) == 2\n",
    "\n",
    "        _indices = [i for i in range(len(labels))]\n",
    "        if randomize:\n",
    "            _indices = torch.randperm(len(_indices))\n",
    "            _indices = _indices.tolist()\n",
    "\n",
    "        _X_batches, _Y_batches = [], []\n",
    "        for _batch in range(len(labels) // batch_size):\n",
    "            _i= _indices[batch_size*_batch : batch_size*(_batch+1)]\n",
    "            _X_batches.append(data[_i,::])\n",
    "            _Y_batches.append(labels[_i,::])\n",
    "\n",
    "        batches = _X_batches, _Y_batches\n",
    "        return batches\n",
    "\n",
    "    def one_hot(self, labels: tuple | list) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Creates a one-hot encoding of given labels.\n",
    "\n",
    "         Parameters:\n",
    "        - labels: tuple or list containing labels.\n",
    "\n",
    "         Returns:\n",
    "        - one_hot_Y: 2D torch Tensor of shape (nr. data points, nr. classes).\n",
    "        \"\"\"\n",
    "        assert type(labels) is tuple or type(labels) is list, f'Unrecognized labels type: should be tuple or list.'\n",
    "        # Creating map from label to integer\n",
    "        self.label_dict = {}\n",
    "        for class_idx, label in enumerate(set(labels)):\n",
    "            self.label_dict[label] = class_idx\n",
    "        # Calculating Number of data points x Nr of classes one-hot encoding of Y\n",
    "        nr_labels = len(list(labels))\n",
    "        nr_unique_labels = len(list(self.label_dict.keys()))\n",
    "        one_hot_Y = torch.zeros(size=(nr_labels, nr_unique_labels))\n",
    "        for i, label in enumerate(labels):\n",
    "            one_hot_Y[i][self.label_dict[label]] = 1\n",
    "        return one_hot_Y\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_pixels(data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Void type function normalizing every value: (0,255) -> (0,1), in input tensor.\n",
    "\n",
    "         Parameters:\n",
    "        - data: 4D torch tensor of data of shape (nr. datapoints, nr. channels, height, width)\n",
    "\n",
    "        \"\"\"\n",
    "        _PIXEL_MAX = 255\n",
    "        for data_point in range(data.shape[0]):\n",
    "            data[data_point] *= 1.0/_PIXEL_MAX\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    \"\"\" Inspired by: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\"\"\"\n",
    "    def __init__(self, channels_in: int,\n",
    "                        nr_classes: int,\n",
    "                 input_dimensions: tuple[int,int]) -> None:\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_dim = input_dimensions\n",
    "        self.nr_classes = nr_classes\n",
    "        self.device = None\n",
    "\n",
    "        # layer 1: convolutional layer\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=channels_in,\n",
    "                                    out_channels=6,\n",
    "                                     kernel_size=(5,5),\n",
    "                                          stride=(1,1),\n",
    "                                         padding=(0,0))\n",
    "        # activation function\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        object_dim1 = self.output_dim(self.input_dim,\n",
    "                                      self.conv1.kernel_size,\n",
    "                                      self.conv1.stride,\n",
    "                                      self.conv1.padding)\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 2: pooling layer\n",
    "        self.pool1 = torch.nn.AvgPool2d(kernel_size=(2,2),\n",
    "                                             stride=(1,1),\n",
    "                                             padding=(0,0))\n",
    "\n",
    "        object_dim2 = self.output_dim(object_dim1,\n",
    "                                      self.pool1.kernel_size,\n",
    "                                      self.pool1.stride,\n",
    "                                      self.pool1.padding)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 3: convolutional layer\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=6,\n",
    "                                    out_channels=16,\n",
    "                                     kernel_size=5,\n",
    "                                          stride=(1,1),\n",
    "                                         padding=(0,0))\n",
    "        # activation function\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        object_dim3 = self.output_dim(object_dim2,\n",
    "                                      self.conv2.kernel_size,\n",
    "                                      self.conv2.stride,\n",
    "                                      self.conv2.padding)\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 4: pooling layer\n",
    "        self.pool2 = torch.nn.AvgPool2d(kernel_size=(2,2),\n",
    "                                             stride=(1,1),\n",
    "                                             padding=(0,0))\n",
    "\n",
    "        object_dim4 = self.output_dim(object_dim3,\n",
    "                                      self.pool2.kernel_size,\n",
    "                                      self.pool2.stride,\n",
    "                                      self.pool2.padding)\n",
    "        # flattening\n",
    "        self.flatten = torch.nn.Flatten(start_dim=1,end_dim=-1)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 5: fully connected layer (lazy means input dim is automatically inferred)\n",
    "        in_vector_length = self.conv2.out_channels*object_dim4[0]*object_dim4[1]\n",
    "        self.lin1 = torch.nn.Linear(in_features=in_vector_length,\n",
    "                                    out_features=120)\n",
    "\n",
    "        # activation function\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 6: fully connected layer (lazy means input dim is automatically inferred)\n",
    "        self.lin2 = torch.nn.Linear(in_features=self.lin1.out_features,\n",
    "                                    out_features=self.nr_classes)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def output_dim(data_size: tuple[int, int],\n",
    "                   kernel_size: tuple[int, int],\n",
    "                   stride_size: tuple[int, int] = (1,1),\n",
    "                   padding_size: tuple[int, int] = (0,0)) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Calculates output shape of array after convolution w. specific\n",
    "        kernel, padding and stride.\n",
    "\n",
    "         Parameters:\n",
    "        - data_size: tuple containing dimension of 2D input array.\n",
    "        - kernel_size: tuple containing dimension of 2D kernel.\n",
    "        - stride_size: tuple containing dimension of 2D stride.\n",
    "        - padding_size: tuple containing dimension of 2D padding.\n",
    "\n",
    "         Returns:\n",
    "        - output_dimensions: tuple containing dimension of resulting 2D array.\n",
    "        \"\"\"\n",
    "\n",
    "        out_height = ((data_size[0] - kernel_size[0] + 2 * padding_size[0]) // stride_size[0]) + 1\n",
    "        out_width = ((data_size[1] - kernel_size[1] + 2 * padding_size[1]) // stride_size[1]) + 1\n",
    "\n",
    "        output_dimensions = (out_height, out_width)\n",
    "        return output_dimensions\n",
    "\n",
    "    def predict(self, forwarded_data:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Takes the raw estimate from a forward pass (nr. data points, nr. classes)\n",
    "        and sets the highest val in each row 1 and the rest zero.\n",
    "\n",
    "         Parameters:\n",
    "        - forwarded_data: torch.Tensor of shape (nr. data points, nr. classes)\n",
    "\n",
    "         Returns:\n",
    "        - _prediction: torc.Tensor of shape (nr. data points, nr. classes)\n",
    "\n",
    "        \"\"\"\n",
    "        assert forwarded_data.shape[1] == self.nr_classes, f'Forwarded data should be of shape (nr. data points, nr. classes).'\n",
    "        assert len(forwarded_data.shape) == 2, f' Forwarded data should be a 2D tensor.'\n",
    "        assert forwarded_data.device.type == self.device.type, f'forwarded_data is on device: {forwarded_data.device.type}, but should be on: {self.device.type}'\n",
    "        for _row in forwarded_data:\n",
    "            _row[_row < torch.max(_row)] = 0\n",
    "            _row[_row == torch.max(_row)] = 1\n",
    "            _row.to(self.device)\n",
    "        assert forwarded_data.device.type == self.device.type, f'_prediction is on device: {forwarded_data.device.type}, but should be on: {self.device.type}'\n",
    "        return forwarded_data\n",
    "\n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Function performing the forward pass on 'data', i.e. sequentially\n",
    "        applying each layer in the network.\n",
    "\n",
    "         Parameters:\n",
    "        - data: 4D torch tensor of shape (nr. data points, nr. channels, height, width).\n",
    "\n",
    "         Returns:\n",
    "        - data: 4D torch tensor of shape (nr. data points, nr. channels, height, width).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(data.shape) == 4, f'Shape of X should be (nr. data points, nr. channels, height, width)'\n",
    "        data = self.conv1(data)\n",
    "        data = self.activation1(data)\n",
    "        data = self.pool1(data)\n",
    "        data = self.conv2(data)\n",
    "        data = self.activation2(data)\n",
    "        data = self.pool2(data)\n",
    "        data = self.flatten(data)\n",
    "        data = self.lin1(data)\n",
    "        data = self.activation3(data)\n",
    "        data = self.lin2(data)\n",
    "        return data\n",
    "\n",
    "    def accuracy(self,y_hat: torch.Tensor,\n",
    "                   y_actual: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Function for calculating the accuracy of 'y_hat' against 'y_actual'\n",
    "        in terms of: nr. agreements / nr. comparisons.\n",
    "\n",
    "         Parameters:\n",
    "        - y_hat: 2D torch tensor of shape (nr. datapoints, nr. classes) only containing 1 and 0.\n",
    "        - y_actual: 2D torch tensor of shape (nr. datapoints, nr. classes) only containing 1 and 0.\n",
    "\n",
    "         Returns:\n",
    "        - _accuracy: float in range (0,1).\n",
    "        \"\"\"\n",
    "        assert y_hat.shape == y_actual.shape, f'Inputs are not of matching shapes.'\n",
    "        assert y_hat.device.type == self.device.type, f'y_hat is on device: {y_hat.device.type}, but should be on: {self.device.type}'\n",
    "        assert y_actual.device.type == self.device.type, f'y_actual is on device: {y_actual.device.type}, but should be on: {self.device.type}'\n",
    "\n",
    "\n",
    "        _nr_equals = torch.sum(y_hat*y_actual)\n",
    "        _accuracy = _nr_equals/y_hat.shape[0]\n",
    "        return _accuracy\n",
    "\n",
    "    def train_network(self,train_data_batches: list[torch.Tensor],\n",
    "                         train_labels_batches: list[torch.Tensor],\n",
    "                            test_data_batches: list[torch.Tensor],\n",
    "                          test_labels_batches: list[torch.Tensor],\n",
    "                                       epochs: int = 10,\n",
    "                                  device_name: str = \"cuda\") -> tuple[list[float],list[float],list[float],list[float]]:\n",
    "\n",
    "        \"\"\"\n",
    "        Function for training the network, e.i. learn the optimal weights for each\n",
    "        layer. Training is done using stochastic gradient descent, with Negative\n",
    "        Log-Likelihood (also known as Cross Entropy) as a loss function.\n",
    "\n",
    "         Parameters:\n",
    "        - train_data_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - train_labels_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - test_data_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - test_labels_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - epochs: nr. of epochs (iteration) used to train.\n",
    "\n",
    "         Returns:\n",
    "        - _train_accuracies: list of floats w. avg. accuracies of training data at given epoch.\n",
    "        - _train_losses: list of floats w. avg. loss of training data at given epoch.\n",
    "        - _test_accuracies: list of floats w. avg. accuracies of test data at given epoch.\n",
    "        - _test_losses: list of floats w. avg. loss of test data at given epoch.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        if device_name != 'cpu':\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(device_name)\n",
    "                # Allocating model weights on GPU\n",
    "                self.to(self.device)\n",
    "                self.train()\n",
    "                print(\"Current torch cuda version: \", torch.version.cuda)\n",
    "                print(\"Cuda devices found by torch: \", torch.cuda.device_count())\n",
    "                print(\"Current cuda device to be used by torch: \", torch.cuda.current_device())\n",
    "                print(\"Name of cuda device: \", torch.cuda.get_device_name(0))\n",
    "                t = torch.cuda.get_device_properties(0).total_memory\n",
    "                print(\"Total memory in cuda device: \", t/1e6, \"MB\")\n",
    "                r = torch.cuda.memory_reserved(0)\n",
    "                print(\"Total memory reserved in cuda device: \", r/1e6, \"MB\")\n",
    "                a = torch.cuda.memory_allocated(0)\n",
    "                print(\"Total memory allocated in cuda device: \", a/1e6, \"MB\")\n",
    "                print(\"Total remaining memory in cuda device:: \", (t-r)/1e6, \"MB\")\n",
    "                # Allocating input on GPU\n",
    "                for _batch in range(len(train_data_batches)):\n",
    "                    train_data_batches[_batch]   = train_data_batches[_batch].to(self.device)\n",
    "                    train_labels_batches[_batch] = train_labels_batches[_batch].to(self.device)\n",
    "                for _batch in range(len(test_data_batches)):\n",
    "                    test_data_batches[_batch]   = test_data_batches[_batch].to(self.device)\n",
    "                    test_labels_batches[_batch] = test_labels_batches[_batch].to(self.device)\n",
    "            else:\n",
    "                self.to(self.device)\n",
    "                self.train()\n",
    "                print(\"No cuda device available to torch - defaulting to cpu.\")\n",
    "\n",
    "        _optimizer = torch.optim.SGD(params=self.parameters(),\n",
    "                                     lr=0.001,\n",
    "                                     momentum=0.9)\n",
    "\n",
    "        _loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        _train_accuracies, _train_losses = [], []\n",
    "        _test_accuracies, _test_losses = [], []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            _counter = 0\n",
    "            _batch_train_acc, _batch_train_loss = 0.0, 0.0\n",
    "            _batch_test_acc,  _batch_test_loss = 0.0, 0.0\n",
    "            # Training against training data\n",
    "            for x_batch, y_batch in zip(train_data_batches,train_labels_batches):\n",
    "                assert x_batch.device.type == self.device.type, f'x_batch is on device: {x_batch.device.type}, but should be on: {self.device.type}'\n",
    "                assert y_batch.device.type == self.device.type, f'y_batch is on device: {y_batch.device.type}, but should be on: {self.device.type}'\n",
    "                # Ensure that parameter gradients are 0.\n",
    "                _optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                _y_hat = self.forward(x_batch)\n",
    "                assert _y_hat.shape[0] == train_labels_batches[0].shape[0]\n",
    "                assert _y_hat.shape[1] == self.nr_classes\n",
    "                # Loss calculation + backprop + optimization\n",
    "                _loss = _loss_function(_y_hat,y_batch)\n",
    "                _loss.backward()\n",
    "                _optimizer.step()\n",
    "                # Saving losses and accuracies\n",
    "                with torch.no_grad():\n",
    "                    if self.device.type != 'cpu':\n",
    "                        _batch_train_acc  += self.accuracy(self.predict(_y_hat),y_batch).item()\n",
    "                    else:\n",
    "                        _batch_train_acc  += self.accuracy(self.predict(_y_hat),y_batch)\n",
    "                    _batch_train_loss += _loss.item()\n",
    "                    _counter += 1\n",
    "            _train_accuracies.append(_batch_train_acc/_counter)\n",
    "            _train_losses.append(_batch_train_loss/_counter)\n",
    "            # Testing model on test data\n",
    "            for x_batch, y_batch in zip(test_data_batches,test_labels_batches):\n",
    "                assert x_batch.device.type == self.device.type, f'x_batch is on device: {x_batch.device.type}, but should be on: {self.device.type}'\n",
    "                assert y_batch.device.type == self.device.type, f'y_batch is on device: {y_batch.device.type}, but should be on: {self.device.type}'\n",
    "                # Forward pass\n",
    "                _y_hat = self.forward(x_batch)\n",
    "                assert _y_hat.shape[0] == train_labels_batches[0].shape[0]\n",
    "                assert _y_hat.shape[1] == self.nr_classes\n",
    "                # Loss calculation\n",
    "                _loss = _loss_function(_y_hat,y_batch)\n",
    "                # Saving losses and accuracies\n",
    "                with torch.no_grad():\n",
    "                    if self.device.type != 'cpu':\n",
    "                        _batch_test_acc  += self.accuracy(self.predict(_y_hat),y_batch).item()\n",
    "                    else:\n",
    "                        _batch_test_acc  += self.accuracy(self.predict(_y_hat),y_batch)\n",
    "                    _batch_test_loss += _loss.item()\n",
    "            _test_accuracies.append(_batch_test_acc/_counter)\n",
    "            _test_losses.append(_batch_test_loss/_counter)\n",
    "\n",
    "        # Removing from GPU and Allocating input on CPU\n",
    "        if self.device.type == \"cuda\":\n",
    "            self.device = torch.device('cpu')\n",
    "            self.to(self.device)\n",
    "            for _batch in range(len(train_data_batches)):\n",
    "                train_data_batches[_batch]   = train_data_batches[_batch].to(self.device)\n",
    "                train_labels_batches[_batch] = train_labels_batches[_batch].to(self.device)\n",
    "            for _batch in range(len(test_data_batches)):\n",
    "                test_data_batches[_batch]   = test_data_batches[_batch].to(self.device)\n",
    "                test_labels_batches[_batch] = test_labels_batches[_batch].to(self.device)\n",
    "\n",
    "        return _train_accuracies, _train_losses, _test_accuracies, _test_losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class NeuralNet2(torch.nn.Module):\n",
    "    \"\"\" Inspired by https://learn.microsoft.com/en-us/windows/ai/windows-ml/tutorials/pytorch-train-model \"\"\"\n",
    "\n",
    "    def __init__(self, channels_in: int,\n",
    "                        nr_classes: int,\n",
    "                 input_dimensions: tuple[int,int]) -> None:\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.input_dim = input_dimensions\n",
    "        self.nr_classes = nr_classes\n",
    "        self.device = None\n",
    "\n",
    "        # layer 1: convolutional layer\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=channels_in,\n",
    "                                    out_channels=12,\n",
    "                                     kernel_size=(5,5),\n",
    "                                          stride=(1,1),\n",
    "                                         padding=(1,1))\n",
    "        # normalization function\n",
    "        self.norm1 = torch.nn.BatchNorm2d(self.conv1.out_channels)\n",
    "        # activation function\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        object_dim1 = self.output_dim(self.input_dim,\n",
    "                                      self.conv1.kernel_size,\n",
    "                                      self.conv1.stride,\n",
    "                                      self.conv1.padding)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 2: convolutional layer\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=self.conv1.out_channels,\n",
    "                                    out_channels=12,\n",
    "                                     kernel_size=(5,5),\n",
    "                                          stride=(1,1),\n",
    "                                         padding=(1,1))\n",
    "        # normalization function\n",
    "        self.norm2 = torch.nn.BatchNorm2d(self.conv2.out_channels)\n",
    "        # activation function\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        object_dim2 = self.output_dim(object_dim1,\n",
    "                                      self.conv2.kernel_size,\n",
    "                                      self.conv2.stride,\n",
    "                                      self.conv2.padding)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 3: pooling layer\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=(2,2),\n",
    "                                             stride=(2,2),\n",
    "                                             padding=(0,0))\n",
    "\n",
    "        object_dim3 = self.output_dim(object_dim2,\n",
    "                                      self.pool1.kernel_size,\n",
    "                                      self.pool1.stride,\n",
    "                                      self.pool1.padding)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 4: convolutional layer\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=self.conv2.out_channels,\n",
    "                                    out_channels=24,\n",
    "                                     kernel_size=(5,5),\n",
    "                                          stride=(1,1),\n",
    "                                         padding=(1,1))\n",
    "        # normalization function\n",
    "        self.norm3 = torch.nn.BatchNorm2d(self.conv3.out_channels)\n",
    "        # activation function\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "        object_dim4 = self.output_dim(object_dim3,\n",
    "                                      self.conv3.kernel_size,\n",
    "                                      self.conv3.stride,\n",
    "                                      self.conv3.padding)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # layer 5: convolutional layer\n",
    "        self.conv4 = torch.nn.Conv2d(in_channels=self.conv3.out_channels,\n",
    "                                    out_channels=24,\n",
    "                                     kernel_size=(5,5),\n",
    "                                          stride=(1,1),\n",
    "                                         padding=(1,1))\n",
    "        # normalization function\n",
    "        self.norm4= torch.nn.BatchNorm2d(self.conv4.out_channels)\n",
    "\n",
    "        # activation function\n",
    "        self.activation4 = torch.nn.ReLU()\n",
    "        object_dim5 = self.output_dim(object_dim4,\n",
    "                                      self.conv4.kernel_size,\n",
    "                                      self.conv4.stride,\n",
    "                                      self.conv4.padding)\n",
    "\n",
    "        #--------------------------------------#\n",
    "\n",
    "        # flattening\n",
    "        self.flatten = torch.nn.Flatten(start_dim=1,end_dim=-1)\n",
    "\n",
    "        #--------------------------------------#\n",
    "        # layer 6: fully connected layer (lazy means input dim is automatically inferred)\n",
    "        in_vector_length = self.conv4.out_channels*object_dim5[0]*object_dim5[1]\n",
    "        self.lin1 = torch.nn.Linear(in_features=in_vector_length,\n",
    "                                    out_features=self.nr_classes)\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def output_dim(data_size: tuple[int, int],\n",
    "                   kernel_size: tuple[int, int],\n",
    "                   stride_size: tuple[int, int] = (1,1),\n",
    "                   padding_size: tuple[int, int] = (0,0)) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Calculates output shape of array after convolution w. specific\n",
    "        kernel, padding and stride.\n",
    "\n",
    "         Parameters:\n",
    "        - data_size: tuple containing dimension of 2D input array.\n",
    "        - kernel_size: tuple containing dimension of 2D kernel.\n",
    "        - stride_size: tuple containing dimension of 2D stride.\n",
    "        - padding_size: tuple containing dimension of 2D padding.\n",
    "\n",
    "         Returns:\n",
    "        - output_dimensions: tuple containing dimension of resulting 2D array.\n",
    "        \"\"\"\n",
    "\n",
    "        out_height = ((data_size[0] - kernel_size[0] + 2 * padding_size[0]) // stride_size[0]) + 1\n",
    "        out_width = ((data_size[1] - kernel_size[1] + 2 * padding_size[1]) // stride_size[1]) + 1\n",
    "\n",
    "        output_dimensions = (out_height, out_width)\n",
    "        return output_dimensions\n",
    "\n",
    "    def predict(self, forwarded_data:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Takes the raw estimate from a forward pass (nr. data points, nr. classes)\n",
    "        and sets the highest val in each row 1 and the rest zero.\n",
    "\n",
    "         Parameters:\n",
    "        - forwarded_data: torch.Tensor of shape (nr. data points, nr. classes)\n",
    "\n",
    "         Returns:\n",
    "        - _prediction: torc.Tensor of shape (nr. data points, nr. classes)\n",
    "\n",
    "        \"\"\"\n",
    "        assert forwarded_data.shape[1] == self.nr_classes, f'Forwarded data should be of shape (nr. data points, nr. classes).'\n",
    "        assert len(forwarded_data.shape) == 2, f' Forwarded data should be a 2D tensor.'\n",
    "        assert forwarded_data.device.type == self.device.type, f'forwarded_data is on device: {forwarded_data.device.type}, but should be on: {self.device.type}'\n",
    "        for _row in forwarded_data:\n",
    "            _row[_row < torch.max(_row)] = 0\n",
    "            _row[_row == torch.max(_row)] = 1\n",
    "            _row.to(self.device)\n",
    "        assert forwarded_data.device.type == self.device.type, f'_prediction is on device: {forwarded_data.device.type}, but should be on: {self.device.type}'\n",
    "        return forwarded_data\n",
    "\n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Function performing the forward pass on 'data', i.e. sequentially\n",
    "        applying each layer in the network.\n",
    "\n",
    "         Parameters:\n",
    "        - data: 4D torch tensor of shape (nr. data points, nr. channels, height, width).\n",
    "\n",
    "         Returns:\n",
    "        - data: 4D torch tensor of shape (nr. data points, nr. channels, height, width).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(data.shape) == 4, f'Shape of X should be (nr. data points, nr. channels, height, width)'\n",
    "\n",
    "        # Layer 1\n",
    "        data = self.conv1(data)\n",
    "        data = self.norm1(data)\n",
    "        data = self.activation1(data)\n",
    "\n",
    "        # Layer 2\n",
    "        data = self.conv2(data)\n",
    "        data = self.norm2(data)\n",
    "        data = self.activation2(data)\n",
    "\n",
    "        # Pool layer\n",
    "        data = self.pool1(data)\n",
    "\n",
    "        # Layer 3\n",
    "        data = self.conv3(data)\n",
    "        data = self.norm3(data)\n",
    "        data = self.activation3(data)\n",
    "\n",
    "        # Layer 4\n",
    "        data = self.conv4(data)\n",
    "        data = self.norm4(data)\n",
    "        data = self.activation4(data)\n",
    "\n",
    "        # Layer 5\n",
    "        data = self.flatten(data)\n",
    "        data = self.lin1(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def accuracy(self,y_hat: torch.Tensor,\n",
    "                   y_actual: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Function for calculating the accuracy of 'y_hat' against 'y_actual'\n",
    "        in terms of: nr. agreements / nr. comparisons.\n",
    "\n",
    "         Parameters:\n",
    "        - y_hat: 2D torch tensor of shape (nr. datapoints, nr. classes) only containing 1 and 0.\n",
    "        - y_actual: 2D torch tensor of shape (nr. datapoints, nr. classes) only containing 1 and 0.\n",
    "\n",
    "         Returns:\n",
    "        - _accuracy: float in range (0,1).\n",
    "        \"\"\"\n",
    "        assert y_hat.shape == y_actual.shape, f'Inputs are not of matching shapes.'\n",
    "        assert y_hat.device.type == self.device.type, f'y_hat is on device: {y_hat.device.type}, but should be on: {self.device.type}'\n",
    "        assert y_actual.device.type == self.device.type, f'y_actual is on device: {y_actual.device.type}, but should be on: {self.device.type}'\n",
    "\n",
    "\n",
    "        _nr_equals = torch.sum(y_hat*y_actual)\n",
    "        _accuracy = _nr_equals/y_hat.shape[0]\n",
    "        return _accuracy\n",
    "\n",
    "    def train_network(self,train_data_batches: list[torch.Tensor],\n",
    "                         train_labels_batches: list[torch.Tensor],\n",
    "                            test_data_batches: list[torch.Tensor],\n",
    "                          test_labels_batches: list[torch.Tensor],\n",
    "                                       epochs: int = 10,\n",
    "                                  device_name: str = \"cuda\") -> tuple[list[float],list[float],list[float],list[float]]:\n",
    "\n",
    "        \"\"\"\n",
    "        Function for training the network, e.i. learn the optimal weights for each\n",
    "        layer. Training is done using stochastic gradient descent, with Negative\n",
    "        Log-Likelihood (also known as Cross Entropy) as a loss function.\n",
    "\n",
    "         Parameters:\n",
    "        - train_data_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - train_labels_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - test_data_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - test_labels_batches: list of 4D torch tensors, each of shape (nr. data points, nr. channels, height, width)\n",
    "        - epochs: nr. of epochs (iteration) used to train.\n",
    "\n",
    "         Returns:\n",
    "        - _train_accuracies: list of floats w. avg. accuracies of training data at given epoch.\n",
    "        - _train_losses: list of floats w. avg. loss of training data at given epoch.\n",
    "        - _test_accuracies: list of floats w. avg. accuracies of test data at given epoch.\n",
    "        - _test_losses: list of floats w. avg. loss of test data at given epoch.\n",
    "\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cpu')\n",
    "        if device_name != 'cpu':\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(device_name)\n",
    "                # Allocating model weights on GPU\n",
    "                self.to(self.device)\n",
    "                self.train()\n",
    "                print(\"Current torch cuda version: \", torch.version.cuda)\n",
    "                print(\"Cuda devices found by torch: \", torch.cuda.device_count())\n",
    "                print(\"Current cuda device to be used by torch: \", torch.cuda.current_device())\n",
    "                print(\"Name of cuda device: \", torch.cuda.get_device_name(0))\n",
    "                t = torch.cuda.get_device_properties(0).total_memory\n",
    "                print(\"Total memory in cuda device: \", t/1e6, \"MB\")\n",
    "                r = torch.cuda.memory_reserved(0)\n",
    "                print(\"Total memory reserved in cuda device: \", r/1e6, \"MB\")\n",
    "                a = torch.cuda.memory_allocated(0)\n",
    "                print(\"Total memory allocated in cuda device: \", a/1e6, \"MB\")\n",
    "                print(\"Total remaining memory in cuda device:: \", (t-r)/1e6, \"MB\")\n",
    "                # Allocating input on GPU\n",
    "                for _batch in range(len(train_data_batches)):\n",
    "                    train_data_batches[_batch]   = train_data_batches[_batch].to(self.device)\n",
    "                    train_labels_batches[_batch] = train_labels_batches[_batch].to(self.device)\n",
    "                for _batch in range(len(test_data_batches)):\n",
    "                    test_data_batches[_batch]   = test_data_batches[_batch].to(self.device)\n",
    "                    test_labels_batches[_batch] = test_labels_batches[_batch].to(self.device)\n",
    "            else:\n",
    "                self.to(self.device)\n",
    "                self.train()\n",
    "                print(\"No cuda device available to torch - defaulting to cpu.\")\n",
    "\n",
    "\n",
    "        _optimizer = torch.optim.Adam(params=self.parameters(),\n",
    "                                     lr=0.001,\n",
    "                                     weight_decay=0.0001)\n",
    "        _loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        _train_accuracies, _train_losses = [], []\n",
    "        _test_accuracies, _test_losses = [], []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            _counter = 0\n",
    "            _batch_train_acc, _batch_train_loss = 0.0, 0.0\n",
    "            _batch_test_acc,  _batch_test_loss = 0.0, 0.0\n",
    "            # Training against training data\n",
    "            for x_batch, y_batch in zip(train_data_batches,train_labels_batches):\n",
    "                assert x_batch.device.type == self.device.type, f'x_batch is on device: {x_batch.device.type}, but should be on: {self.device.type}'\n",
    "                assert y_batch.device.type == self.device.type, f'y_batch is on device: {y_batch.device.type}, but should be on: {self.device.type}'\n",
    "                # Ensure that parameter gradients are 0.\n",
    "                _optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                _y_hat = self.forward(x_batch)\n",
    "                assert _y_hat.shape[0] == train_labels_batches[0].shape[0]\n",
    "                assert _y_hat.shape[1] == self.nr_classes\n",
    "                # Loss calculation + backprop + optimization\n",
    "                _loss = _loss_function(_y_hat,y_batch)\n",
    "                _loss.backward()\n",
    "                _optimizer.step()\n",
    "                # Saving losses and accuracies\n",
    "                with torch.no_grad():\n",
    "                    if self.device.type != 'cpu':\n",
    "                        _batch_train_acc  += self.accuracy(self.predict(_y_hat),y_batch).item()\n",
    "                    else:\n",
    "                        _batch_train_acc  += self.accuracy(self.predict(_y_hat),y_batch)\n",
    "                    _batch_train_loss += _loss.item()\n",
    "                    _counter += 1\n",
    "            _train_accuracies.append(_batch_train_acc/_counter)\n",
    "            _train_losses.append(_batch_train_loss/_counter)\n",
    "            # Testing model on test data\n",
    "            for x_batch, y_batch in zip(test_data_batches,test_labels_batches):\n",
    "                assert x_batch.device.type == self.device.type, f'x_batch is on device: {x_batch.device.type}, but should be on: {self.device.type}'\n",
    "                assert y_batch.device.type == self.device.type, f'y_batch is on device: {y_batch.device.type}, but should be on: {self.device.type}'\n",
    "                # Forward pass\n",
    "                _y_hat = self.forward(x_batch)\n",
    "                assert _y_hat.shape[0] == train_labels_batches[0].shape[0]\n",
    "                assert _y_hat.shape[1] == self.nr_classes\n",
    "                # Loss calculation\n",
    "                _loss = _loss_function(_y_hat,y_batch)\n",
    "                # Saving losses and accuracies\n",
    "                with torch.no_grad():\n",
    "                    if self.device.type != 'cpu':\n",
    "                        _batch_test_acc  += self.accuracy(self.predict(_y_hat),y_batch).item()\n",
    "                    else:\n",
    "                        _batch_test_acc  += self.accuracy(self.predict(_y_hat),y_batch)\n",
    "                    _batch_test_loss += _loss.item()\n",
    "            _test_accuracies.append(_batch_test_acc/_counter)\n",
    "            _test_losses.append(_batch_test_loss/_counter)\n",
    "\n",
    "        # Removing from GPU and Allocating input on CPU\n",
    "        if self.device.type == \"cuda\":\n",
    "            self.device = torch.device('cpu')\n",
    "            self.to(self.device)\n",
    "            for _batch in range(len(train_data_batches)):\n",
    "                train_data_batches[_batch]   = train_data_batches[_batch].to(self.device)\n",
    "                train_labels_batches[_batch] = train_labels_batches[_batch].to(self.device)\n",
    "            for _batch in range(len(test_data_batches)):\n",
    "                test_data_batches[_batch]   = test_data_batches[_batch].to(self.device)\n",
    "                test_labels_batches[_batch] = test_labels_batches[_batch].to(self.device)\n",
    "        return _train_accuracies, _train_losses, _test_accuracies, _test_losses\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial nr. of batches:  362\n",
      "Final nr. of batches:  25\n",
      "Shape of single batch:  torch.Size([2, 3, 520, 520])\n",
      "my_X_test_batches:  7\n"
     ]
    }
   ],
   "source": [
    "# Set up the dataset.\n",
    "image_directory = \"Insects\"\n",
    "annotations_file_directory = \"insects.csv\"\n",
    "dataset = MyCustomImageDataset(annotations_file_directory, image_directory)\n",
    "\n",
    "\n",
    "# Preparing data for learning (normalization, one-hot encoding and batching)\n",
    "my_batch_size, my_data_fraction, my_test_fraction = 2, 0.07, 0.3\n",
    "prepped_data = DataPrep(dataset=dataset,\n",
    "                        batch_size=my_batch_size,\n",
    "                        data_fraction=my_data_fraction,\n",
    "                        test_fraction=my_test_fraction)\n",
    "prepped_data.prepare()\n",
    "my_X_train_batches, my_Y_train_batches = prepped_data.train_X, prepped_data.train_Y\n",
    "my_X_test_batches, my_Y_test_batches = prepped_data.test_X, prepped_data.test_Y\n",
    "print(\"Shape of single batch: \",my_X_train_batches[0].shape)\n",
    "print(\"my_X_test_batches: \", len(my_X_test_batches))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Creating instance of neural network\n",
    "my_nr_channels = my_X_train_batches[0].shape[1]\n",
    "my_nr_classes = my_Y_train_batches[0].shape[1]\n",
    "my_input_dimensions = (my_X_train_batches[0].shape[2], my_X_train_batches[0].shape[3])\n",
    "\n",
    "my_net = NeuralNet2(channels_in=my_nr_channels,\n",
    "                   nr_classes=my_nr_classes,\n",
    "                   input_dimensions=my_input_dimensions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:52<00:00, 11.27s/it]\n"
     ]
    }
   ],
   "source": [
    "train_accuracies, train_losses, test_accuracies, test_losses = my_net.train_network(\n",
    "                                                        train_data_batches=my_X_train_batches,\n",
    "                                                      train_labels_batches=my_Y_train_batches,\n",
    "                                                         test_data_batches=my_X_test_batches,\n",
    "                                                       test_labels_batches=my_Y_test_batches,\n",
    "                                                                    epochs=10,\n",
    "                                                               device_name='cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 600x400 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAF4CAYAAAArCuGxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA54ElEQVR4nO3deXhTVd4H8G+SNmnTJXRNF1paZKdQliIWRFQqdesMjL4CIhQc8VHhfYEO8wqyi1B0hGERYUBR8RVhcESZAXGwCChUlkIRRixbS8vSjaWhaemS3PeP0kDoAqlNck/6/TzPfSQ39yYnAXu/Ped3zlVIkiSBiIiIWjSlsxtAREREzsdAQERERAwERERExEBAREREYCAgIiIiMBAQERERGAiIiIgIDAREREQEBgIiIiICAwERERGhCYFgz549SEpKQlhYGBQKBb766qtGj//yyy/x2GOPISgoCL6+voiPj8e3337b1PYSERGRHdgcCIxGI2JjY7FixYp7On7Pnj147LHHsG3bNmRkZOCRRx5BUlISjhw5YnNjiYiIyD4Uv+XmRgqFAps3b8aQIUNsOq9r164YNmwYZs2a1dS3JiIiombk5ug3NJvNuH79Ovz9/Rs8pqKiAhUVFVbnXLlyBQEBAVAoFI5oJhERkUuQJAnXr19HWFgYlMqGBwYcHgjeffddlJaW4rnnnmvwmNTUVMydO9eBrSIiInJteXl5aN26dYPPO3TIYP369Rg3bhy+/vprJCQkNHjcnT0EJSUliIyMRF5eHnx9fZvaXCIiohbHYDAgIiIC165dg06na/A4h/UQbNiwAS+99BI2bdrUaBgAAI1GA41GU2e/r68vAwEREVET3G3I3SHrEHz++ecYO3YsPv/8czz11FOOeEsiIiKygc09BKWlpTh9+rTlcXZ2NjIzM+Hv74/IyEhMmzYNFy5cwLp16wDUDBMkJydj6dKl6Nu3L/Lz8wEAnp6ejXZdEBERkePY3ENw6NAh9OzZEz179gQApKSkoGfPnpYphJcuXUJubq7l+NWrV6O6uhrjx49HaGioZZs4cWIzfQQiIiL6rX5TUaGjGAwG6HQ6lJSUNFhDYDKZUFVV5eCWUVO5u7tDpVI5uxlERC7vXq6hgBOmHdpDaWkpzp8/DwGyDd2kUCjQunVreHt7O7spREQEFwgEJpMJ58+fh1arRVBQEBcuEoAkSSgqKsL58+fRvn179hQQEcmA8IGgqqoKkiQhKCgInp6ezm4O3aOgoCDk5OSgqqqKgYCISAZc5vbH7BkQC/++iIjkxWUCARERETUdAwERERExELiKqKgoLFmyxNnNICIiQQlfVCiqhx9+GD169Gi2i/jBgwfh5eXVLK9FREQtDwOBjEmSBJPJBDe3u/81BQUFOaBFRETkqlxuyECSJJRVVjtlu9eFkcaMGYPdu3dj6dKlUCgUUCgUyMnJwa5du6BQKPDNN9+gd+/e0Gg0+PHHH3HmzBn8/ve/h16vh7e3N/r06YPvvvvO6jXvHDJQKBT44IMPMHToUGi1WrRv3x5btmxptF2ffvop4uLi4OPjg5CQEDz//PMoLCy0OuY///kPnn76afj6+sLHxwcDBgzAmTNnLM+vXbsWXbt2hUajQWhoKCZMmHBP3wkRETmXy/UQlFeZ0GXWt05571/eTIRWffevdOnSpTh58iRiYmLw5ptvArg1Lx8Apk6dinfffRdt27aFn58f8vLy8OSTT2L+/PnQaDRYt24dkpKSkJWVhcjIyAbfZ+7cuXjnnXfwl7/8BcuXL8fIkSNx7tw5+Pv713t8VVUV5s2bh44dO6KwsBApKSkYM2YMtm3bBgC4cOECHnroITz88MPYuXMnfH19sXfvXlRXVwMAVq5ciZSUFCxcuBBPPPEESkpKsHfvXlu+QiIichKXCwQi0Ol0UKvV0Gq1CAkJqfP8m2++iccee8zy2N/fH7GxsZbH8+bNw+bNm7Fly5ZGfwMfM2YMRowYAQBYsGABli1bhgMHDuDxxx+v9/gXX3zR8ue2bdti2bJl6NOnD0pLS+Ht7Y0VK1ZAp9Nhw4YNcHd3BwB06NDBcs5bb72FP/3pT1Y3rurTp8/dvg4iIpIBlwsEnu4q/PJmotPeuznExcVZPS4tLcWcOXOwdetWXLp0CdXV1SgvL7e6q2R9unfvbvmzl5cXfH196wwB3C4jIwNz5szB0aNHcfXqVZjNZgBAbm4uunTpgszMTAwYMMASBm5XWFiIixcvYtCgQbZ8VCIikgmXCwQKheKeuu3l7M7ZAlOmTMGOHTvw7rvvol27dvD09MSzzz6LysrKRl/nzgu3QqGwXOTvZDQakZiYiMTERHz22WcICgpCbm4uEhMTLe/T2NLQXDaaiEhsLldUKAq1Wg2TyXRPx+7duxdjxozB0KFD0a1bN4SEhFjqDZrLr7/+isuXL2PhwoUYMGAAOnXqVKc3oXv37vjhhx/qvc20j48PoqKikJaW1qztIiIix2AgcJKoqCjs378fOTk5KC4ubvA3dwBo3749vvzyS2RmZuLo0aN4/vnnGz2+KSIjI6FWq7F8+XKcPXsWW7Zswbx586yOmTBhAgwGA4YPH45Dhw7h1KlT+PTTT5GVlQUAmDNnDhYtWoRly5bh1KlTOHz4MJYvX96s7SQiIvtgIHCSKVOmQKVSoUuXLpbu+YYsXrwYfn5+6NevH5KSkpCYmIhevXo1a3uCgoLw8ccfY9OmTejSpQsWLlyId9991+qYgIAA7Ny5E6WlpRg4cCB69+6NNWvWWIYmkpOTsWTJErz//vvo2rUrnn76aZw6dapZ20lERPahkO518rwTGQwG6HQ6lJSUwNfX1+q5GzduIDs7G9HR0fDw8HBSC8lW/HsjInKMxq6ht2MPARERETEQEBEREQMBERERgYGAiIiIwEBAREREYCAgIiIiMBAQERERGAiIiIgIDAREREQEBgIiIiICA4HTPPzww5g0aVKzvuaYMWMwZMiQZn1NIiJqGRgIiIiIyAUDgSQBlUbnbPd4n6gxY8Zg9+7dWLp0KRQKBRQKBXJycgAAx48fxxNPPAFvb2/o9XqMGjUKxcXFlnO/+OILdOvWDZ6enggICEBCQgKMRiPmzJmDTz75BF9//bXlNXft2lXv+2/fvh0PPvggWrVqhYCAADz99NM4c+aM1THnz5/HiBEj4O/vDy8vL8TFxWH//v2W5//5z3+iT58+8PDwQGBgIIYOHWrb3xMREcmKm7Mb0OyqyoAFYc557zcuAmqvux62dOlSnDx5EjExMXjzzTcB1Nx++Nq1a3j00Ufx0ksv4a9//SvKy8vx+uuv47nnnsPOnTtx6dIljBgxAu+88w6GDh2K69ev44cffoAkSZgyZQpOnDgBg8GAjz76CADg7+9f7/sbjUakpKSge/fuKC0txaxZszB06FBkZmZCqVRabm8cHh6OLVu2ICQkBIcPH4bZbAYAbN26FUOHDsX06dOxbt06VFZWYtu2bc30JRIRkTO4XiAQgE6ng1qthlarRUhIiGX/e++9h549e2LBggWWfWvXrkVERAROnjyJ0tJSVFdX4w9/+APatGkDAOjWrZvlWE9PT1RUVFi9Zn2eeeYZq8dr165FUFAQfvnlF8TExGD9+vUoKirCwYMHLaGiXbt2luPnz5+P4cOHY+7cuZZ9sbGxTfgmiIhILlwvELhra35Td9Z7/wZHjx7F999/D29v7zrPnTlzBoMHD8agQYPQrVs3JCYmYvDgwXj22Wfh5+dn0/ucOnUKs2bNwv79+1FcXGz5zT83NxcxMTHIzMxEz549G+xhyMzMxLhx42z/gEREJFuuFwgUinvqtpej0tJSJCUl4e23367zXGhoKFQqFXbs2IF9+/bh3//+N5YvX47p06dj//79iI6Ovuf3SUpKQps2bbBmzRqEhYXBbDYjJiYGlZWVAGp6Ghpzt+eJiEg8rldUKAi1Wg2TyWS1r1evXvjPf/6DqKgotGvXzmrz8qoJOQqFAv3798fcuXNx5MgRqNVqbN68ucHXvNPly5eRlZWFGTNmYNCgQejcuTOuXr1qdUz37t2RmZmJK1eu1Psa3bt3R1paWlM/OhERyZDNgWDPnj1ISkpCWFgYFAoFvvrqq7ues2vXLvTq1QsajQbt2rXDxx9/3ISmupaoqCjs378fOTk5lm778ePH48qVKxgxYgQOHjyIM2fO4Ntvv8XYsWNhMpmwf/9+LFiwAIcOHUJubi6+/PJLFBUVoXPnzpbX/Pnnn5GVlYXi4mJUVVXVeV8/Pz8EBARg9erVOH36NHbu3ImUlBSrY0aMGIGQkBAMGTIEe/fuxdmzZ/GPf/wD6enpAIDZs2fj888/x+zZs3HixAkcO3as3l4NIiISh82BwGg0IjY2FitWrLin47Ozs/HUU0/hkUceQWZmJiZNmoSXXnoJ3377rc2NdSVTpkyBSqVCly5dEBQUhNzcXISFhWHv3r0wmUwYPHgwunXrhkmTJqFVq1ZQKpXw9fXFnj178OSTT6JDhw6YMWMGFi1ahCeeeAIAMG7cOHTs2BFxcXEICgrC3r1767yvUqnEhg0bkJGRgZiYGEyePBl/+ctfrI5Rq9X497//jeDgYDz55JPo1q0bFi5cCJVKBaBmUaVNmzZhy5Yt6NGjBx599FEcOHDA/l8aERHZjUKS7nHyfH0nKxTYvHlzo6vjvf7669i6dSuOHz9u2Td8+HBcu3YN27dvv6f3MRgM0Ol0KCkpga+vr9VzN27cQHZ2NqKjo+Hh4dGkz0GOx783IiLHaOwaeju7FxWmp6cjISHBal9iYmKjy/ZWVFSgoqLC8thgMNireUQkA5Ik4WpZFXIuG3HushE5xWU4d9mIc1fKkHelDJXVZmc3kchhkmLDMH9ot7sf2MzsHgjy8/Oh1+ut9un1ehgMBpSXl9dbsZ6ammo1x52IxCdJEoquVyDnctmtC//lMuTefHz9RrWzm0gkC+VVjReH24sspx1OmzbNqtDNYDAgIiLCiS0ionthNku4ZLiBc8U1F/tzl404ZwkAZXf9QReq80CbAC2iArzQJsALbQK0iPTXQqtWOegTEDmft4dzLs12f9eQkBAUFBRY7SsoKICvr2+D89k1Gg00Go29m0ZETVBtMuPCtfKbv93fuvDnXC5D7l2695UKINzP8+YFv+6F38OdF34iZ7F7IIiPj6+zzv2OHTsQHx/frO/zG2ojyQn49yVvFdUm5F0pt/yGf+62C//5q+WoNjf89+euUiDCT4s2AVq0CfBC1M3/tgnQorWfFmo3Ln9CJEc2B4LS0lKcPn3a8jg7OxuZmZnw9/dHZGQkpk2bhgsXLmDdunUAgFdeeQXvvfce/vd//xcvvvgidu7cib///e/YunVrs3yA2qlwlZWVXEFPILWrItb+/ZHjlVeacO5K3Qt+TnEZLpaUN3rzTo2bst4LflSAF0J1HnBT8aJPJBqbA8GhQ4fwyCOPWB7XjvUnJyfj448/xqVLl5Cbm2t5Pjo6Glu3bsXkyZOxdOlStG7dGh988AESExObofmAm5sbtFotioqK4O7uDqWSP4jkzmw2o6ioCFqtFm5usixjcSmV1WbsOVmErILrVhf+AkNFo+d5qVU1F/zAWxf+SP+ax3ofDyiVCgd9AiJyhN+0DoGj3G0OZWVlJbKzsy036SH5UyqViI6OhlqtdnZTXFZ+yQ2s338O6w/kobi0/ou/r4cbogO96vym3ybAC4HeaigUvOgTiU426xA4glqtRvv27S3d0CR/arWavTl2IEkS9mdfwbr0HHz7nwKYbo71B/to8GC7QEQFell19bfSMpARUQ2XCARAzW+cXPGOWipjRTU2H7mAT9PPIavgumX//dH+SI6PwuCuerhzXJ+IGuEygYCoJTpbVIpPfzqHLw6dx/WKmoV9PN1VGNorHKPj26BTSMPdg0REt2MgIBKMySxh56+FWJeegx9OFVv2Rwd64YUH2uDZ3q2h83R3YguJSEQMBESCuGqsxMZDefg0/RwuXCsHACgUwKBOwRgVH4UB7QJZ+U9ETcZAQA5TbTJjxlfHYTJLGNgxCA+2C2RR2z04dr4En6Tn4J9HL6Li5iqArbTuGBYXgRceaIMIf62TW0hEroCBgBzm0Lmr2HAwDwCwKeM8lAogNqIVBnYIwkMdghDbuhVU/A0XQM1KgduOXcIn+84hM++aZX9MuC9Gx0fhd7FhXOaXiJoVAwE5TOnNu9n5ad0R7OOBrILrOJJ7DUdyr2HJd6fQSuuOB9sFYmCHIAzsEIRg35Y3a+TitXJ8tv8cNhzIw2VjzTRad5UCT3ULxeh+UegZ0YprAxCRXTAQkMOU3bzTXacQX3z+8gO4VFKOPSeLsPtkEX48VYxrZVX418+X8K+fL908zgcDO9aEg7g2/i67Br4kSUg/cxnr0s/h37/ko/Y2AaE6D4zsG4lhfSIR5MObfRGRfTEQkMOUV9b0ENTeyjZU54lhfWoueNUmM46ev4bdWUXYfaoYP5+/hl/zr+PX/Ov42+6z0KpV6HdfwM3eg2BEBog/bl5aUY0vD5/HuvRzOF1Yatkf3zYAyf3aIKGznvcEICKHYSAghymrrOkh8Kzn3vZuKiV6t/FH7zb+SBncEVeMlfjhVE3vwZ6TxSgurcB3Jwrx3YlCAP9BdKAXHmofiIEdg/BA2wBo1eL8Uz5deB2fpp/DPw5fQOnNtQO81Cr8oVdrjIpvgw56Hye3kIhaInF+ipLwagOBtp5AcCd/LzV+3yMcv+8RDrNZwol8A3afLMLurCJknLuK7GIjsouN+CT9HNQqJe6P9sdDHQIxsEMwOui9ZTfOXm0y47sThfj0pxzsPX3Zsr9tkBeS46Pwh17h8PHg2gFE5DwMBOQwZZYhA9v+2SmVCnQN06FrmA6vPdwO129UIf3M5ZqAcLII56+W48fTxfjxdDEWbPsVIb4elnDwYLtA6LTOu9BeLq3AhoN5+Oync7hYcqPm8yiAhM56JPeLQr/7AmQXXoioZWIgIIdpbMjAFj4e7hjcNQSDu4ZAkiRkFxst4eCns5eRb7iBvx86j78fqpna2DPSDw+1D8LAjkHoFq5zyNTGzLxrWLcvB//6+RIqTTVrB/h7qTG8TwRGPtAG4a087d4GIiJbMBCQw5TXDhk04/x5hUKBtkHeaBvkjbH9o3GjyoSDOVdqihNPFuFUYSkyzl1Fxrmr+Ot3J+GndceA9jXrHjzUIRDBPs03tfFGlQn/+vkS1qXn4OfzJZb9sRGtkBzfBk92C+XaAUQkWwwE5DDN1UPQGA93FQa0D8KA9kGYgZp5/bdPbbxaVoUtRy9iy9GLAIAuob6WqY29Iv2aNLXx/NUy/N9Pudh4MBdXy6oAAGo3JZK6h2F0fBvERrRqxk9IRGQfDATkMLeKCh33zy6slSeG3x+J4fdHospkRmbeNUtA+Pl8CX65ZMAvlwxYuesMvNQq9LttYaTGlgSWJAk/ni7GuvRzSDtRYFk7ILyVJ154oA2G9YmAvxeXZSYicTAQkMOUV1mvQ+Bo7iol+kT5o0+UP/40uCMul1bgx9PF2J1VhD2nilBcWokdvxRgxy8FAIC2gV54qENN7cED0QHwVKtguFGFf2Scx6c/ncPZIqPltQe0D8SoB9pgUGc9l18mIiExEJDD2DLt0BECvDVWUxt/uWSwFCcePncVZ4uNOFtsxMf7cqB2U6JH61Y4frHE8jm8NW54tndrvPBAG7QL9nbypyEi+m0YCMhhyp0wZHCvlEoFYsJ1iAnXYfwj7WC4UYV9py9jz6matQ8uXCvHgZwrAIAOem+Mio/C0J7h8NbI77MQETUFf5qRwziiqLC5+Hq44/GYEDweUzO18UyREQdzriA60At9o/25dgARuRwGAnIYuQ0Z3CuFQoF2wd4cFiAil8Y7p5DD3HlzIyIikg8GAnIISZIstz8WYciAiKilYSAgh7hRZYZ0c66+HIsKiYhaOgYCcojaGxsBgCeX7yUikh0GAnKI2oJCjZuSC/cQEckQAwE5RHmVmDMMiIhaCgYCcghn3MeAiIjuHQMBOURtDQFnGBARyRMDATlE7bLFXgwERESyxEBADiHSssVERC0RAwE5hJxvbERERAwE5CCsISAikjcGAnKI2mWLtVyUiIhIlhgIyCHKBb3TIRFRS8FAQA5hrKgtKmQNARGRHDUpEKxYsQJRUVHw8PBA3759ceDAgUaPX7JkCTp27AhPT09ERERg8uTJuHHjRpMaTGIqr+Ktj4mI5MzmQLBx40akpKRg9uzZOHz4MGJjY5GYmIjCwsJ6j1+/fj2mTp2K2bNn48SJE/jwww+xceNGvPHGG7+58SSOMg4ZEBHJms2BYPHixRg3bhzGjh2LLl26YNWqVdBqtVi7dm29x+/btw/9+/fH888/j6ioKAwePBgjRoxotFehoqICBoPBaiOxcR0CIiJ5sykQVFZWIiMjAwkJCbdeQKlEQkIC0tPT6z2nX79+yMjIsASAs2fPYtu2bXjyyScbfJ/U1FTodDrLFhERYUszSYZYVEhEJG82VXgVFxfDZDJBr9db7dfr9fj111/rPef5559HcXExHnzwQUiShOrqarzyyiuNDhlMmzYNKSkplscGg4GhQHCWdQjcWVRIRCRHdp9lsGvXLixYsADvv/8+Dh8+jC+//BJbt27FvHnzGjxHo9HA19fXaiOxsYaAiEjebPp1LTAwECqVCgUFBVb7CwoKEBISUu85M2fOxKhRo/DSSy8BALp16waj0YiXX34Z06dPh1LJmY8tQfnNhYm8NAwERERyZNPVWK1Wo3fv3khLS7PsM5vNSEtLQ3x8fL3nlJWV1bnoq1Q1FwVJkmxtLwnKUlTIIQMiIlmy+adzSkoKkpOTERcXh/vvvx9LliyB0WjE2LFjAQCjR49GeHg4UlNTAQBJSUlYvHgxevbsib59++L06dOYOXMmkpKSLMGAXB+LComI5M3mQDBs2DAUFRVh1qxZyM/PR48ePbB9+3ZLoWFubq5Vj8CMGTOgUCgwY8YMXLhwAUFBQUhKSsL8+fOb71OQrEmSZCkqZCAgIpInhSRAv73BYIBOp0NJSQkLDAV0o8qETjO3AwCOzRkMHw93J7eIiKjluNdrKCv6yO5qhwsAQMt7GRARyRIDAdmd8eZwgdpNCZVS4eTWEBFRfRgIyO5YUEhEJH8MBGR3lkWJ3BkIiIjkioGA7I43NiIikj8GArK78qraKYcsKCQikisGArI79hAQEckfAwHZXW0g8GIgICKSLQYCsrtbsww4ZEBEJFcMBGR3HDIgIpI/BgKyu3Lex4CISPYYCMju2ENARCR/DARkd2VVtQsTsYaAiEiuGAjI7soqOGRARCR3DARkdxwyICKSPwYCsrvyKt7ciIhI7hgIyO7KeLdDIiLZYyAgu7s1ZMCiQiIiuWIgILvjOgRERPLHQEB2Z+khcGcgICKSKwYCsrvaexl4aThkQEQkVwwEZFeSJN1amIhDBkREssVAQHZVaTLDZJYAcB0CIiI5YyAgu6odLgAALWsIiIhki4GA7Kq2oFCtUsJNxX9uRERyxZ/QZFdlN6cccriAiEjeGAjIrrhKIRGRGBgIyK54YyMiIjEwEJBdlbOHgIhICAwEZFeWIQN3LkpERCRnDARkVywqJCISAwMB2VU5VykkIhICAwHZ1a1ZBhwyICKSMwYCsitOOyQiEgMDAdlV+c0aAgYCIiJ5YyAgu+I6BEREYmhSIFixYgWioqLg4eGBvn374sCBA40ef+3aNYwfPx6hoaHQaDTo0KEDtm3b1qQGk1i4DgERkRhsrvTauHEjUlJSsGrVKvTt2xdLlixBYmIisrKyEBwcXOf4yspKPPbYYwgODsYXX3yB8PBwnDt3Dq1atWqO9pPMGS3TDllUSEQkZzb/lF68eDHGjRuHsWPHAgBWrVqFrVu3Yu3atZg6dWqd49euXYsrV65g3759cHd3BwBERUX9tlaTMG4tTMQeAiIiObNpyKCyshIZGRlISEi49QJKJRISEpCenl7vOVu2bEF8fDzGjx8PvV6PmJgYLFiwACaTqcH3qaiogMFgsNpITBwyICISg02BoLi4GCaTCXq93mq/Xq9Hfn5+veecPXsWX3zxBUwmE7Zt24aZM2di0aJFeOuttxp8n9TUVOh0OssWERFhSzNJRlhUSEQkBrvPMjCbzQgODsbq1avRu3dvDBs2DNOnT8eqVasaPGfatGkoKSmxbHl5efZuJtnJrZUKWUNARCRnNv2UDgwMhEqlQkFBgdX+goIChISE1HtOaGgo3N3doVLd+g2xc+fOyM/PR2VlJdRqdZ1zNBoNNBqNLU0jmSrjOgREREKwqYdArVajd+/eSEtLs+wzm81IS0tDfHx8vef0798fp0+fhtlstuw7efIkQkND6w0D5Fo4ZEBEJAabhwxSUlKwZs0afPLJJzhx4gReffVVGI1Gy6yD0aNHY9q0aZbjX331VVy5cgUTJ07EyZMnsXXrVixYsADjx49vvk9BslVbVOjFIQMiIlmz+af0sGHDUFRUhFmzZiE/Px89evTA9u3bLYWGubm5UCpv5YyIiAh8++23mDx5Mrp3747w8HBMnDgRr7/+evN9CpKlymozqs0SAPYQEBHJnUKSJMnZjbgbg8EAnU6HkpIS+Pr6Ors5dI9KyqoQ++a/AQCn5j8BdxVXyiYicrR7vYbyJzTZTVlVTUGhu0rBMEBEJHP8KU12Yyko5CqFRESyx0BAdlNWwTUIiIhEwUBAdsM1CIiIxMFAQHZTVsU1CIiIRMFAQHbDGxsREYmDgYDs5tYqhawhICKSOwYCspvy2hoCzjIgIpI9BgKymzIOGRARCYOBgOyGNzYiIhIHAwHZTfnNWQZeGtYQEBHJHQMB2U3tOgRcqZCISP4YCMhuWENARCQOBgKyG65DQEQkDgYCshsj1yEgIhIGAwHZTTnvZUBEJAwGArIbTjskIhIHAwHZjaWGgLMMiIhkj4GA7ObWLAPWEBARyR0DAdmNZR0CDhkQEckeAwHZTe1KhSwqJCKSPwYCsosqkxlVJgkAAwERkQgYCMguausHANYQEBGJgIGA7KJ2hoGbUgG1G/+ZERHJHX9Sk12woJCISCwMBGQXvLEREZFYGAjILrgGARGRWBgIyC4sQwZcpZCISAgMBGQXvPUxEZFYGAjILnhjIyIisTAQkF2UcZVCIiKhMBCQXZTfrCFgUSERkRgYCMguOGRARCQWBgKyC0tRIWcZEBEJgYGA7IILExERiYWBgOzCEgg0rCEgIhIBAwHZRXlVbVEhewiIiETQpECwYsUKREVFwcPDA3379sWBAwfu6bwNGzZAoVBgyJAhTXlbEoilqJA1BEREQrA5EGzcuBEpKSmYPXs2Dh8+jNjYWCQmJqKwsLDR83JycjBlyhQMGDCgyY0lcZRV8F4GREQisTkQLF68GOPGjcPYsWPRpUsXrFq1ClqtFmvXrm3wHJPJhJEjR2Lu3Llo27btXd+joqICBoPBaiOxlHHIgIhIKDYFgsrKSmRkZCAhIeHWCyiVSEhIQHp6eoPnvfnmmwgODsYf//jHe3qf1NRU6HQ6yxYREWFLM0kGuA4BEZFYbAoExcXFMJlM0Ov1Vvv1ej3y8/PrPefHH3/Ehx9+iDVr1tzz+0ybNg0lJSWWLS8vz5Zmkgzw5kZERGKx6wDv9evXMWrUKKxZswaBgYH3fJ5Go4FGo7Fjy8jeuA4BEZFYbAoEgYGBUKlUKCgosNpfUFCAkJCQOsefOXMGOTk5SEpKsuwzm801b+zmhqysLNx3331NaTfJXLllyIBFhUREIrBpyECtVqN3795IS0uz7DObzUhLS0N8fHyd4zt16oRjx44hMzPTsv3ud7/DI488gszMTNYGuKhqkxmVpprgx6WLiYjEYPOvbykpKUhOTkZcXBzuv/9+LFmyBEajEWPHjgUAjB49GuHh4UhNTYWHhwdiYmKszm/VqhUA1NlPrqP21scAiwqJiERhcyAYNmwYioqKMGvWLOTn56NHjx7Yvn27pdAwNzcXSiUXQGzJaocLlApA48Z/C0REIlBIkiQ5uxF3YzAYoNPpUFJSAl9fX2c3h+4iu9iIR97dBR+NG47NTXR2c4iIWrR7vYby1zdqdmWVNYsScbiAiEgcDATU7LgGARGReBgIqNkZOeWQiEg4DATU7MoreR8DIiLRMBBQs+MqhURE4mEgoGZnubERFyUiIhIGAwE1OxYVEhGJh4GAml0ZiwqJiITDQEDNrqyKRYVERKJhIKBmxyEDIiLxMBBQs7s1ZMBAQEQkCgYCana1PQRerCEgIhIGAwE1O97LgIhIPPwVTkRndwGXzzi7FQ3qdzUbYSojOp3PAiRd3QO8gwG/aMA/GlB7Ob6BRERUBwOBaC6fAdb93tmtaNQ4AHAHkHlza4x3CODf9uYWbf1fj3rCBBER2QUDgWhKztf8V+MLRD/k3LY04IfTxTBWmNAnyg8BXmrrJyUJKM0HrpwFyq/W/Lk0H8jdV/eFtAG3hYXbNr9oQOsPKBSO+UBERC0AA4FoKo01/w3sAAz/zLltacDrqWm4WHUDXw/uj4CIVg0fWHYFuJoNXMmuCQhXzt76s7EQKLtcs50/WPdcje62HoW21r0M3nqGBSIiGzEQiKaytOa/Gm/ntqMRZVX3uA6B1r9mC+9d97mK63cEhZth4Wo2YLgAVJQAlzJrtju5e90MC9F1exZ8wwEla2mJiO7EQCCa2kCglnEgaI51CDQ+QGj3mu1OVeXA1Zw7wsLNwFCSB1QZgYLjNdudVBrAL6qeuoW2gC4CUPF/CSJqmfjTTzQV8g4EJrOEymozAEBrr3UI3D2B4M41252qK4FruXXDwtXsmhBhqgCKs2q2OyndgFaR9dcttIoE3DT2+TxERDLAQCCa2hoCmU7Xq12DAHDS0sVuaiCwXc12J1M1YDhft17hytmasFB949bjOhSArnVNzwURkT11fAIYNMvhb8tAIBrLkIE8A0HtKoUKBaBxk9lYvcqtZrjALwq471Hr58xm4Pql+ochrmbXfO8lec5oNRG1NKGxTnlbBgLRWIoK5fmbam39gNZdBYVIlf5KJaALr9miB1g/J0mAsagmHFTfcE77iKjl8NY75W0ZCEQj+yGD2oJCF/qnpVDUrK7oHezslhAR2Y3M+nTprmQeCMqramoIvDS8jwERkUgYCERTcb3mvzKdZWDpIXBnICAiEgkDgWgsPQTyDgROmWFARERNxkAgGpmvVFg77dBuaxAQEZFdMBCIRuY1BM2ySiERETkcA4FoZL50cTmHDIiIhMRAIBJJkv3SxawhICISEwOBSKorAKnmgiv7IQN31hAQEYmEgUAktcMFgGwDQbmlqJA9BEREImEgEEltIHDXAkp5XnBZVEhEJCYGApHIfIYBAJRVsYaAiEhEDAQikXlBIcBZBkREompSIFixYgWioqLg4eGBvn374sCBAw0eu2bNGgwYMAB+fn7w8/NDQkJCo8dTI2Q+5RDgwkRERKKyORBs3LgRKSkpmD17Ng4fPozY2FgkJiaisLCw3uN37dqFESNG4Pvvv0d6ejoiIiIwePBgXLhw4Tc3vsURYMiAPQRERGKyORAsXrwY48aNw9ixY9GlSxesWrUKWq0Wa9eurff4zz77DK+99hp69OiBTp064YMPPoDZbEZaWtpvbnyLI/NliwEWFRIRicqmQFBZWYmMjAwkJCTcegGlEgkJCUhPT7+n1ygrK0NVVRX8/f0bPKaiogIGg8FqIwjRQ3BrYSIOGRARicSmQFBcXAyTyQS9Xm+1X6/XIz8//55e4/XXX0dYWJhVqLhTamoqdDqdZYuIiLClma7LcutjH+e2oxFlXIeAiEhIDp1lsHDhQmzYsAGbN2+Gh4dHg8dNmzYNJSUlli0vL8+BrZQxgXoIPN0ZCIiIRGJTv25gYCBUKhUKCgqs9hcUFCAkJKTRc999910sXLgQ3333Hbp3797osRqNBhqNxpamtQwyDwQms4SKajMA9hAQEYnGph4CtVqN3r17WxUE1hYIxsfHN3jeO++8g3nz5mH79u2Ii4tremtbusqbQwYyLSosv7koEcAaAiIi0dj8UzslJQXJycmIi4vD/fffjyVLlsBoNGLs2LEAgNGjRyM8PBypqakAgLfffhuzZs3C+vXrERUVZak18Pb2hre3PC9ssmXpIZDn91ZbP6BQAB7uXPOKiEgkNgeCYcOGoaioCLNmzUJ+fj569OiB7du3WwoNc3NzoVTeuhisXLkSlZWVePbZZ61eZ/bs2ZgzZ85va31LY1mpUJ5DBuW31Q8oFAont4aIiGzRpH7dCRMmYMKECfU+t2vXLqvHOTk5TXkLqo/sewi4KBERkajYrysSmS9dzEWJiIjExUAgEpmvVGhZttidBYVERKJhIBCJzKcdWhYl0rCHgIhINAwEIpF5IKiddsgaAiIi8TAQiMJsvq2GQJ5LFxsramcZcMiAiEg0DASiqCq79WeZ9hDwPgZEROJiIBBFbe+AQgm4ezq3LQ0o57RDIiJhMRCI4vY1CGS66E9ZFacdEhGJioFAFJXyXqUQYA8BEZHIGAhEUSHvRYmA22sIWFRIRCQaBgJRyHzKIXDbSoXu7CEgIhINA4Eoam99LOMeAg4ZEBGJi4FAFLU9BDJdthjgvQyIiETGQCAKEYYMLCsVsoaAiEg0DASiEKCosPxmUaEXewiIiITDQCAKmd/6GOCQARGRyBgIRCHCkEElhwyIiETFQCCK2h4CWRcV8l4GRESiYiAQhcxXKjSbJdyoMgPgkAERkYgYCEQh86LC8pszDAD2EBARiYiBQBS339xIhmrrBwDAw42BgIhINAwEopB5UWH5bcsWK5XyvBsjERE1jIFAFLVLF2t8nNuOBpRVsaCQiEhkDASikHkPAdcgICISGwOBKCrkPcuANzYiIhIbA4EITFWAqaLmzzIvKvTkokREREJiIBBB7XABIONAwPsYEBGJjIFABLWLEindATe1c9vSAA4ZEBGJjYFABLU9BDJettjIIQMiIqExEIhAgDsd1t76WOvOHgIiIhExEIhA5ssWA5x2SEQkOgYCEch8DQLg9lsfMxAQEYmIgUAEMr/TIcCiQiIi0TEQiKA2EMh02WIAKKtiUSERkcgYCEQgwJCBpaiQPQREREJiIBCBzJctBlhDQEQkuiYFghUrViAqKgoeHh7o27cvDhw40OjxmzZtQqdOneDh4YFu3bph27ZtTWpsiyXAtMOy225/TERE4rE5EGzcuBEpKSmYPXs2Dh8+jNjYWCQmJqKwsLDe4/ft24cRI0bgj3/8I44cOYIhQ4ZgyJAhOH78+G9ufIshQCC4VVTIGgIiIhHZHAgWL16McePGYezYsejSpQtWrVoFrVaLtWvX1nv80qVL8fjjj+PPf/4zOnfujHnz5qFXr1547733fnPjWwwBViosq6qpIeA6BEREYrIpEFRWViIjIwMJCQm3XkCpREJCAtLT0+s9Jz093ep4AEhMTGzweACoqKiAwWCw2lo0IYoKa3oIvDQMBEREIrIpEBQXF8NkMkGv11vt1+v1yM/Pr/ec/Px8m44HgNTUVOh0OssWERFhSzNdT8X1mv/KeMjAWHFzyMCdQwZERCKS5SyDadOmoaSkxLLl5eU5u0nOZekhkGcgMJsllFdx6WIiIpHZ9OtcYGAgVCoVCgoKrPYXFBQgJCSk3nNCQkJsOh4ANBoNNBqNLU1zbTIfMrhRbbL8mdMOiYjEZFMPgVqtRu/evZGWlmbZZzabkZaWhvj4+HrPiY+PtzoeAHbs2NHg8VQPy0qF8uwhqJ1yCHDaIRGRqGwe8E1JSUFycjLi4uJw//33Y8mSJTAajRg7diwAYPTo0QgPD0dqaioAYOLEiRg4cCAWLVqEp556Chs2bMChQ4ewevXq5v0krkzm0w5rCwo93JVQKhVObg0RETWFzYFg2LBhKCoqwqxZs5Cfn48ePXpg+/btlsLB3NxcKJW3Oh769euH9evXY8aMGXjjjTfQvn17fPXVV4iJiWm+T+HKJEn2KxWWcQ0CIiLhKSRJkpzdiLsxGAzQ6XQoKSmBr6+vs5vjWFU3gPk3Z2lMzQM85Pf5j+RexdD39yG8lSf2Tn3U2c0hIqLb3Os1VJazDOg2tQWFgGx7CHjrYyIi8TEQyF3lzTUI3DwBpTwvuLyxERGR+BgI5E6IZYu5BgERkegYCORO5gWFAFBeWXMfAxYVEhGJi4FA7ixTDn2c245GcMiAiEh8DARyJ/NVCgEGAiIiV8BAIHeV8h8yKOOQARGR8BgI5E6EosJKFhUSEYmOgUDuZL5sMXDbOgS8jwERkbAYCOROgFkG7CEgIhIfA4HcWYoK5dtDwHsZEBGJj4FA7mpXKpRxD0F5VW1RIXsIiIhExUAgd5aiQvmvQ8AhAyIicTEQyJ0A6xDw5kZEROJjIJA7gYoKGQiIiMTFQCB3Ai1d7OnOokIiIlEJ8RNckiQAgMFgcHJLnMBgACokoOLmn2Wo9HoJzJVmmCqMcm0iEVGLVXvtrL2WNkQh3e0IGTh//jwiIiKc3QwiIiJh5eXloXXr1g0+L0QgMJvNuHjxInx8fKBQKJzdHKczGAyIiIhAXl4efH19nd2cFoPfu3Pwe3cOfu/OYY/vXZIkXL9+HWFhYVAqG64UEGLIQKlUNppqWipfX1/+j+oE/N6dg9+7c/B7d47m/t51Ot1dj2FRIRERETEQEBEREQOBkDQaDWbPng2NRuPsprQo/N6dg9+7c/B7dw5nfu9CFBUSERGRfbGHgIiIiBgIiIiIiIGAiIiIwEBAREREYCAQRmpqKvr06QMfHx8EBwdjyJAhyMrKcnazWpyFCxdCoVBg0qRJzm6Ky7tw4QJeeOEFBAQEwNPTE926dcOhQ4ec3SyXZjKZMHPmTERHR8PT0xP33Xcf5s2bd9c18Mk2e/bsQVJSEsLCwqBQKPDVV19ZPS9JEmbNmoXQ0FB4enoiISEBp06dsnu7GAgEsXv3bowfPx4//fQTduzYgaqqKgwePBhGo9HZTWsxDh48iL/97W/o3r27s5vi8q5evYr+/fvD3d0d33zzDX755RcsWrQIfn5+zm6aS3v77bexcuVKvPfeezhx4gTefvttvPPOO1i+fLmzm+ZSjEYjYmNjsWLFinqff+edd7Bs2TKsWrUK+/fvh5eXFxITE3Hjxg27tovTDgVVVFSE4OBg7N69Gw899JCzm+PySktL0atXL7z//vt466230KNHDyxZssTZzXJZU6dOxd69e/HDDz84uyktytNPPw29Xo8PP/zQsu+ZZ56Bp6cn/u///s+JLXNdCoUCmzdvxpAhQwDU9A6EhYXhT3/6E6ZMmQIAKCkpgV6vx8cff4zhw4fbrS3sIRBUSUkJAMDf39/JLWkZxo8fj6eeegoJCQnObkqLsGXLFsTFxeG//uu/EBwcjJ49e2LNmjXObpbL69evH9LS0nDy5EkAwNGjR/Hjjz/iiSeecHLLWo7s7Gzk5+db/azR6XTo27cv0tPT7freQtzciKyZzWZMmjQJ/fv3R0xMjLOb4/I2bNiAw4cP4+DBg85uSotx9uxZrFy5EikpKXjjjTdw8OBB/M///A/UajWSk5Od3TyXNXXqVBgMBnTq1AkqlQomkwnz58/HyJEjnd20FiM/Px8AoNfrrfbr9XrLc/bCQCCg8ePH4/jx4/jxxx+d3RSXl5eXh4kTJ2LHjh3w8PBwdnNaDLPZjLi4OCxYsAAA0LNnTxw/fhyrVq1iILCjv//97/jss8+wfv16dO3aFZmZmZg0aRLCwsL4vbcAHDIQzIQJE/Cvf/0L33//PW8J7QAZGRkoLCxEr1694ObmBjc3N+zevRvLli2Dm5sbTCaTs5vokkJDQ9GlSxerfZ07d0Zubq6TWtQy/PnPf8bUqVMxfPhwdOvWDaNGjcLkyZORmprq7Ka1GCEhIQCAgoICq/0FBQWW5+yFgUAQkiRhwoQJ2Lx5M3bu3Ino6GhnN6lFGDRoEI4dO4bMzEzLFhcXh5EjRyIzMxMqlcrZTXRJ/fv3rzOt9uTJk2jTpo2TWtQylJWVQam0viyoVCqYzWYntajliY6ORkhICNLS0iz7DAYD9u/fj/j4eLu+N4cMBDF+/HisX78eX3/9NXx8fCxjSTqdDp6enk5unevy8fGpU6fh5eWFgIAA1m/Y0eTJk9GvXz8sWLAAzz33HA4cOIDVq1dj9erVzm6aS0tKSsL8+fMRGRmJrl274siRI1i8eDFefPFFZzfNpZSWluL06dOWx9nZ2cjMzIS/vz8iIyMxadIkvPXWW2jfvj2io6Mxc+ZMhIWFWWYi2I1EQgBQ7/bRRx85u2ktzsCBA6WJEyc6uxku75///KcUExMjaTQaqVOnTtLq1aud3SSXZzAYpIkTJ0qRkZGSh4eH1LZtW2n69OlSRUWFs5vmUr7//vt6f54nJydLkiRJZrNZmjlzpqTX6yWNRiMNGjRIysrKsnu7uA4BERERsYaAiIiIGAiIiIgIDAREREQEBgIiIiICAwERERGBgYCIiIjAQEBERERgICAiIiIwEBBRM1MoFPjqq6+c3QwishEDAZGLGDNmDBQKRZ3t8ccfd3bTiEgAvLkRkQt5/PHH8dFHH1nt02g0TmqNfVVVVcHd3d3ZzSByGewhIHIhGo0GISEhVpufn5/leYVCgZUrV+KJJ56Ap6cn2rZtiy+++MLqNY4dO4ZHH30Unp6eCAgIwMsvv4zS0lKrY9auXYuuXbtCo9EgNDQUEyZMsHq+uLgYQ4cOhVarRfv27bFly5ZG2x0VFYUFCxbgxRdfhI+PDyIjI63ubJiTkwOFQoGNGzdi4MCB8PDwwGeffdbUr4mI6sFAQNTCzJw5E8888wyOHj2KkSNHYvjw4Thx4gQAwGg0IjExEX5+fjh48CA2bdqE7777zuqCv3LlSowfPx4vv/wyjh07hi1btqBdu3ZW7zF37lw899xz+Pnnn/Hkk09i5MiRuHLlSqPtWrRoEeLi4nDkyBG89tprePXVV5GVlWV1zNSpUzFx4kScOHECiYmJzfSNEBEA3v6YyFUkJydLKpVK8vLystrmz59vOQaA9Morr1id17dvX+nVV1+VJEmSVq9eLfn5+UmlpaWW57du3SoplUopPz9fkiRJCgsLk6ZPn95gOwBIM2bMsDwuLS2VAEjffPNNg+e0adNGeuGFFyyPzWazFBwcLK1cuVKSJEnKzs6WAEhLliy5l6+CiJqANQRELuSRRx7BypUrrfb5+/tbPY6Pj6/zODMzEwBw4sQJxMbGwsvLy/J8//79YTabkZWVBYVCgYsXL2LQoEGNtqN79+6WP3t5ecHX1xeFhYX3fI5CoUBISEidc+Li4hp9DSJqOgYCIhfi5eVVp/u+OXl6et7TcXcW+ykUCpjN5t98zu1BhYiaF2sIiFqYn376qc7jzp07AwA6d+6Mo0ePwmg0Wp7fu3cvlEolOnbsCB8fH0RFRSEtLc2hbSYi+2MgIHIhFRUVyM/Pt9qKi4utjtm0aRPWrl2LkydPYvbs2Thw4IClaHDkyJHw8PBAcnIyjh8/ju+//x7//d//jVGjRkGv1wMA5syZg0WLFmHZsmU4deoUDh8+jOXLlzv8s27evBmdOnVy+PsSuSoOGRC5kO3btyM0NNRqX8eOHfHrr79aHs+dOxcbNmzAa6+9htDQUHz++efo0qULAECr1eLbb7/FxIkT0adPH2i1WjzzzDNYvHix5fzk5GTcuHEDf/3rXzFlyhQEBgbi2WefdcwHvE1JSUmdWQhE1HQKSZIkZzeCiBxDoVBg8+bNGDJkiLObQkQywyEDIiIiYiAgIiIi1hAQtSgcISSihrCHgIiIiBgIiIiIiIGAiIiIwEBAREREYCAgIiIiMBAQERERGAiIiIgIDAREREQE4P8BwDdMBg5Pi6QAAAAASUVORK5CYII=\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(6,4))\n",
    "epochs = [i+1 for i in range(len(train_accuracies))]\n",
    "ax.plot(epochs,train_accuracies,label=\"train acc\")\n",
    "#ax.plot(epochs,train_losses,label=\"train loss\")\n",
    "ax.plot(epochs,test_accuracies,label=\"test acc\")\n",
    "#ax.plot(epochs,test_losses,label=\"test loss\")\n",
    "ax.set_ylim(-0.1,1.2)\n",
    "ax.set_xlabel(\"Epoch nr.\")\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def test_model(model: NeuralNet,\n",
    "               data: DataPrep,\n",
    "               nr_batches: int) -> None:\n",
    "    # Generating random indices\n",
    "    rand_indices = []\n",
    "    while len(rand_indices) < nr_batches:\n",
    "        _rand_int = torch.randint(low=0,high=len(data.Y),size=(1,)).item()\n",
    "        if _rand_int not in rand_indices:\n",
    "            rand_indices.append(_rand_int)\n",
    "    # Predicting w. model and checking against true labels\n",
    "    batch_counter = 0\n",
    "    correct_counter = 0\n",
    "    with torch.no_grad():\n",
    "        for _index in rand_indices:\n",
    "            mod = model.forward(data.X[_index])\n",
    "            y_hat  = model.predict(mod)\n",
    "            y_real = data.Y[_index]\n",
    "            print(\"\\n ---  Random Batch: \", batch_counter + 1, \" ---\")\n",
    "            for _data_point in range(y_hat.shape[0]):\n",
    "                pred = list(data.label_dict.keys())[torch.where(y_hat[_data_point]==1)[0]]\n",
    "                actual = list(data.label_dict.keys())[torch.where(y_real[_data_point]==1)[0]]\n",
    "                print(\"  ##| Prediction: \", pred,\" |--| Actual: \",actual,\" |##\")\n",
    "                if pred == actual:\n",
    "                    correct_counter += 1\n",
    "            batch_counter += 1\n",
    "    print(\"\\n #####| \", correct_counter, \"/\", batch_counter * data.batch_size, \" predicted correctly |#####\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---  Random Batch:  1  ---\n",
      "  ##| Prediction:  Andrena fulva  |--| Actual:  Andrena fulva  |##\n",
      "  ##| Prediction:  Panurgus banksianus  |--| Actual:  Panurgus banksianus  |##\n",
      "\n",
      " ---  Random Batch:  2  ---\n",
      "  ##| Prediction:  Panurgus banksianus  |--| Actual:  Panurgus banksianus  |##\n",
      "  ##| Prediction:  Lasioglossum punctatissimum  |--| Actual:  Lasioglossum punctatissimum  |##\n",
      "\n",
      " ---  Random Batch:  3  ---\n",
      "  ##| Prediction:  Panurgus banksianus  |--| Actual:  Panurgus banksianus  |##\n",
      "  ##| Prediction:  Andrena fulva  |--| Actual:  Andrena fulva  |##\n",
      "\n",
      " #####|  6 / 6  predicted correctly |#####\n"
     ]
    }
   ],
   "source": [
    "test_model(my_net,prepped_data,3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}